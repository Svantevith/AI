{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.Tensor()\n",
    "type(t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Attributes\n",
    "The dtype, which is torch.float32 in our case, specifies the type of the data that is contained within the tensor.\n",
    "Tensors contain uniform (of the same type) numerical data with one of these types:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(t.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The device, cpu in our case, specifies the device (CPU or GPU) where the tensor's data is allocated.\n",
    "This determines where tensor computations for the given tensor will be performed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(t.device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The layout, strided in our case, specifies how the tensor is stored in memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.strided\n"
     ]
    }
   ],
   "source": [
    "print(t.layout)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As neural network programmers, we need to be aware of the following:\n",
    "\n",
    "1) Tensors contain data of a uniform type (dtype).\n",
    "2) Tensor computations between tensors depend on the dtype and the device.\n",
    "\n",
    "These are the primary ways of creating tensor objects (instances of the torch.Tensor class),\n",
    "with data (array-like) in PyTorch. We’ll begin by just creating a tensor with each of the options and see what we get.\n",
    "We’ll start by creating some data. We can use a Python list, or sequence, but numpy.ndarrays are going to be the more\n",
    "common option, so we’ll go with a numpy.ndarray like so:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data = np.array(\n",
    "    [1, 2, 3]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "numpy.ndarray.dtype gives information about the type of data stored inside (uniform!)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    }
   ],
   "source": [
    "print(data.dtype)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "type() gives information about the type of the object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let’s create our tensors with each of these options 1-4, and have a look at what we get:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "o1 = torch.Tensor(data)\n",
    "o2 = torch.tensor(data)\n",
    "o3 = torch.as_tensor(data)\n",
    "o4 = torch.from_numpy(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of the options (o1, o2, o3, o4) appear to have produced the same tensors except for the first one.\n",
    "The first option (o1) has dots after the number indicating that the numbers are floats, while the next three options\n",
    "have a type of int32."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(o4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creation Options Without Data\n",
    "Here are some other creation options that are available.\n",
    "\n",
    "We have the torch.eye() function which returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
    "The name eye() is connected to the idea of an identity matrix , which is a square matrix with ones on the\n",
    "main diagonal and zeros everywhere else."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    torch.eye(2)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have the torch.zeros() function that creates a tensor of zeros with the shape of specified shape argument."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(\n",
    "    [2, 2]\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Similarly, we have the torch.ones() function that creates a tensor of ones."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(\n",
    "    [2, 2]\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also have the torch.rand() function that creates a tensor with a shape of the specified argument\n",
    "whose values are random."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3107, 0.9423],\n",
      "        [0.8829, 0.4106]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(\n",
    "    [2, 2]\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Creation Operations: What's The Difference?\n",
    "Uppercase/Lowercase: torch.Tensor() Vs torch.tensor()\n",
    "The first option with the uppercase T is the constructor of the torch.Tensor class, and the second option\n",
    "is what we call a factory function that constructs torch.Tensor objects and returns them to the caller.\n",
    "You can think of the torch.tensor() function as a factory that builds tensors given some parameter inputs.\n",
    "\n",
    "Okay. That’s the difference between the uppercase T and the lower case t, but which way is better between these two?\n",
    "The answer is that it’s fine to use either one. However, the factory function torch.tensor() has better documentation\n",
    "and more configuration options, so it gets the winning spot at the moment.\n",
    "\n",
    "The difference here arises in the fact that the torch.Tensor() constructor uses the default dtype\n",
    "when building the tensor. We can verify the default dtype using the torch.get_default_dtype() method:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.get_default_dtype())\n",
    "print(o1.dtype == torch.get_default_dtype())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The other calls choose a dtype based on the incoming data. This is called type inference. The dtype is inferred\n",
    "based on the incoming data. Note that the dtype can also be explicitly set for these calls by specifying the dtype\n",
    "as an argument:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "o2 = torch.tensor(data, dtype=torch.float32)\n",
    "o3 = torch.as_tensor(data, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "With torch.Tensor(), we are unable to pass a dtype to the constructor. This is an example of the\n",
    "torch.Tensor() constructor lacking in configuration options. This is one of the reasons to go with the\n",
    "torch.tensor() factory function for creating our tensors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sharing Memory For Performance: Copy Vs Share"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old data: [1 2 3]\n",
      "New data: [0 2 3]\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1., 2., 3.])\n",
      "tensor([0, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print('Old data:', data)\n",
    "data[0] = 0\n",
    "print('New data:', data)\n",
    "print(o1)\n",
    "print(o2)\n",
    "print(o3)\n",
    "print(o4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch.Tensor() and torch.tensor() copy their input data while torch.as_tensor() and torch.from_numpy()\n",
    "share their input data in memory with the original input object.\n",
    "\n",
    "    • Share (pass by reference, original): torch.as_tensor(), torch.from_numpy()\n",
    "    • Copy (pass by value, copy): torch.Tensor(), torch.tensor()\n",
    "\n",
    "If we have a torch.Tensor and we want to convert it to a numpy.ndarray, we do it like so:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1 = o1.numpy()\n",
    "type(o1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This establishes that torch.as_tensor() and torch.from_numpy() both share memory with their input data.\n",
    "However, which one should we use, and how are they different?\n",
    "\n",
    "The torch.from_numpy() function only accepts numpy.ndarrays, while the torch.as_tensor() function accepts a wide\n",
    "variety of array-like objects including other PyTorch tensors. For this reason, torch.as_tensor() is the winning\n",
    "choice in the memory sharing game.\n",
    "\n",
    "Some things to keep in mind about memory sharing (it works where it can):\n",
    "\n",
    "    1) Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU\n",
    "    to the GPU when a GPU is being used.\n",
    "\n",
    "    2) The memory sharing of as_tensor() doesn’t work with built-in Python data structures like lists.\n",
    "\n",
    "    3) The as_tensor() call requires developer knowledge of the sharing feature. This is necessary so we don’t\n",
    "    inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects.\n",
    "\n",
    "    4) The as_tensor() performance improvement will be greater if there are a lot of back and forth operations between\n",
    "    numpy.ndarray objects and tensor objects. However, if there is just a single load operation, there shouldn’t be much\n",
    "    impact from a performance perspective."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Operation Types\n",
    "We have the following high-level categories of operations:\n",
    "\n",
    "    1) Reshaping operations\n",
    "\n",
    "    2) Element-wise operations\n",
    "\n",
    "    3) Reduction operations\n",
    "\n",
    "    4) Access operations\n",
    "\n",
    "Reshaping operations are perhaps the most important type of tensor operations. This is because, like we mentioned in\n",
    "the post where we introduced tensors, the shape of a tensor gives us something concrete we can use to shape an\n",
    "intuition for our tensors.\n",
    "\n",
    "This is very similar to how a baker uses dough to produce, say, a pizza. The dough is the input used to create\n",
    "an output, but before the pizza is produced there is usually some form of reshaping of the input that is required."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "t = torch.tensor(\n",
    "    data=[\n",
    "        [1, 1, 1, 1],\n",
    "        [2, 2, 2, 2],\n",
    "        [3, 3, 3, 3]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In PyTorch, we have two ways to get the shape:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)\n",
    "print(t.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Typically, after we know a tensor’s shape, we can deduce a couple of things. First, we can deduce the tensor's rank.\n",
    "The rank of a tensor is equal to the length of the tensor's shape (rank is the number of dimensions of the tensor,\n",
    "it is described as the length - number of elements, in the array representing shape of the tensor)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "t_rank = len(t.shape)\n",
    "print(t_rank)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also deduce the number of elements contained within the tensor. The number of elements inside a tensor\n",
    "(12 in our case) is equal to the product of the shape's component values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12)\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(t.shape).prod())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In PyTorch, there is a dedicated function for this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(t.numel())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The number of elements contained within a tensor is important for reshaping because the reshaping must account for\n",
    "the total number of elements present. Reshaping changes the tensor's shape but not the underlying data.\n",
    "Our tensor has 12 elements, so any reshaping must account for exactly 12 elements."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reshaping A Tensor In PyTorch\n",
    "Let’s look now at all the ways in which this tensor t can be reshaped without changing the rank,\n",
    "while maintaining prior number of elements:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "t.reshape([4, 3])\n",
    "t.reshape([6, 2])\n",
    "t.reshape([12, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [2.],\n        [2.],\n        [2.],\n        [2.],\n        [3.],\n        [3.],\n        [3.],\n        [3.]])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the intuitive words rows and columns when we are dealing with a rank 2 tensor.\n",
    "The underlying logic is the same for higher dimensional tenors even though we may not be able to use the intuition\n",
    "of rows and columns in higher dimensional spaces. For example:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1., 1., 1.],\n         [1., 2., 2.]],\n\n        [[2., 2., 3.],\n         [3., 3., 3.]]])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(2, 2, 3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this example, we increase the rank to 3, and so we lose the rows and columns concept. However, the product of the\n",
    "shape's components (2,2,3) still has to be equal to the number of elements in the original tensor (12)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Changing Shape By Squeezing And Unsqueezing\n",
    "    1) Squeezing a tensor removes the dimensions or axes that have a length of one.\n",
    "    2) Unsqueezing a tensor adds a dimension with a length of one.\n",
    "These functions allow us to expand or shrink the rank (number of dimensions) of our tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "torch.Size([12])\n",
      "torch.Size([1, 1, 12])\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape([1, 12]).shape)\n",
    "# We can see that the axis=0 was removed, because of it's length=1\n",
    "print(t.reshape([1, 12]).squeeze().shape)\n",
    "# We can see that the axis=0 with length=1 was added\n",
    "print(t.reshape([1, 12]).unsqueeze(dim=0).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flatten A Tensor\n",
    "Flattening a tensor means to remove all of the dimensions except for one, having a shape that is equal to the total\n",
    "number of elements contained in the tensor.\n",
    "\n",
    "Let’s create a Python function called flatten():"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def flatten(t_in):\n",
    "    # Reshape the tensor to have a single row and corresponding number of columns (rank=2)\n",
    "    t_out = t_in.reshape(1, -1)\n",
    "    # squeeze the tensor, so we get rid of the axis=0 (rank=1)\n",
    "    t_out = t_out.squeeze()\n",
    "    return t_out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "create a 2 dimensional tensor containing ones"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      " torch.Size([4, 3])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]) \n",
      " torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "ex_t = torch.ones([4, 3])\n",
    "print(ex_t, '\\n', ex_t.shape)\n",
    "ex_t = flatten(ex_t)\n",
    "print(ex_t, '\\n', ex_t.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In a future post when we begin building a convolutional neural network, we will see the use of this flatten()\n",
    "function. We'll see that flatten operations are required when passing an output tensor from a convolutional layer to\n",
    "a linear layer (CNN->Dense Layer).\n",
    "\n",
    "In these examples, we have flattened the entire tensor, however, it is possible to flatten only specific parts\n",
    "of a tensor. For example, suppose we have a tensor of shape [2,1,28,28] for a CNN. This means that we have a batch\n",
    "of 2 grayscale images with height and width dimensions of 28 x 28, respectively.\n",
    "\n",
    "Here, we can specifically flatten the two images. To get the following shape: [2,1,784]. We could also squeeze off\n",
    "the channel axes to get the following shape: [2,784]."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Concatenating Tensors\n",
    "We combine tensors using the cat() function, and the resulting tensor will have a shape that depends on the shape\n",
    "of the two input tensors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4]\n",
    "])\n",
    "t2 = torch.tensor([\n",
    "    [5, 6],\n",
    "    [7, 8]\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can combine t1 and t2 row-wise (axis=0, stack vertically by row) in the following way:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6],\n",
      "        [7, 8]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.cat(\n",
    "    (t1, t2), dim=0\n",
    ")\n",
    "print(t3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "or we can combine them column-wise (axis=1) like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "t3 = torch.cat(\n",
    "    (t1, t2), dim=1\n",
    ")\n",
    "print(t3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flattening An Entire Tensor\n",
    "A tensor flatten operation is a common operation inside convolutional neural networks.\n",
    "This is because convolutional layer outputs that are passed to fully connected layers must be flatted out before\n",
    "the fully connected layer will accept the input.\n",
    "\n",
    "To flatten a tensor, we need to have at least two axes. This makes it so that we are starting with something\n",
    "that is not already flat. Let’s look now at a hand written image of an eight from the MNIST dataset.\n",
    "This image has 2 distinct dimensions, height and width.\n",
    "\n",
    "The height and width are 18 x 18 respectively. These dimensions tell us that this is a cropped image because the\n",
    "MNIST dataset contains 28 x 28 images. Let’s see now how these two axes of height and width are flattened out into a\n",
    "single axis of length 324.\n",
    "\n",
    "In this example, we are flattening the entire tensor image, but what if we want to only flatten specific axes within the tensor?\n",
    "This is typically required when working with CNNs.\n",
    "Let’s see how we can flatten out specific axes of a tensor in code with PyTorch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flattening Specific Axes Of A Tensor\n",
    "In the post on CNN input tensor shape, we learned how tensor inputs to a convolutional neural network\n",
    "typically have 4 axes, one for batch size, one for color channels, and one each for height and width.\n",
    "NCHW - (Batch size, Color channel, Height, Width)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building A Tensor Representation For A Batch Of Images"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "t1 = torch.tensor(\n",
    "    np.full((4, 4), 1)\n",
    ")\n",
    "t2 = torch.tensor(\n",
    "    np.full((4, 4), 2)\n",
    ")\n",
    "t3 = torch.tensor(\n",
    "    np.full((4, 4), 3)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each of these has a shape of 4 x 4 (2d), so we have three rank=2 tensors. For our purposes here, we’ll consider these\n",
    "to be three 4 x 4 images that well use to create a batch that can be passed to a CNN.\n",
    "\n",
    "Remember, batches are represented using a single tensor, so we’ll need to combine these three tensors\n",
    "into a single larger tensor that has three axes instead of 2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "t = torch.stack(\n",
    "    (t1, t2, t3)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The axis with a length of 3 represents the batch size while the axes of length 4 represent the height and width\n",
    "respectively."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(t.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "At this point, we have a rank-3 tensor that contains a batch of three 4 x 4 images. All we need to do now to get\n",
    "this tensor into a form that a CNN expects is add an axis for the color channels. We basically have an implicit\n",
    "single color channel for each of these image tensors, so in practice, these would be grayscale images.\n",
    "\n",
    "A CNN will expect to see an explicit color channel axis, so let’s add one by reshaping this tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 4, 4])\n",
      "Tensor:\n",
      " tensor([[[[1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1],\n",
      "          [1, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2],\n",
      "          [2, 2, 2, 2]]],\n",
      "\n",
      "\n",
      "        [[[3, 3, 3, 3],\n",
      "          [3, 3, 3, 3],\n",
      "          [3, 3, 3, 3],\n",
      "          [3, 3, 3, 3]]]], dtype=torch.int32)\n",
      "First image: \n",
      " tensor([[[1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1],\n",
      "         [1, 1, 1, 1]]], dtype=torch.int32)\n",
      "First color channel of the first image: \n",
      " tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "First row of pixels in the first color channel of the first image:\n",
      " tensor([1, 1, 1, 1], dtype=torch.int32)\n",
      "First pixel of the first image:\n",
      " tensor(1, dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "t = t.reshape(3, 1, 4, 4)\n",
    "print(t.shape)\n",
    "print('Tensor:\\n', t)\n",
    "print('First image: \\n', t[0])\n",
    "print('First color channel of the first image: \\n', t[0][0])\n",
    "print('First row of pixels in the first color channel of the first image:\\n', t[0][0][0])\n",
    "print('First pixel of the first image:\\n', t[0][0][0][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flattening The Tensor Batch\n",
    "Alright. Let’s see how to flatten the images in this batch. Remember the whole batch is a single tensor that will be\n",
    "passed to the CNN, so we don’t want to flatten the whole thing. We only want to flatten the image tensors\n",
    "within the batch tensor.\n",
    "Let’s flatten the whole thing first just to see what it will look like."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "       dtype=torch.int32)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "       dtype=torch.int32)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "       dtype=torch.int32)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "       dtype=torch.int32)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(t.reshape(1, -1)[0])\n",
    "print(t.reshape(1, -1).squeeze())\n",
    "print(t.reshape(-1))\n",
    "print(t.view(t.numel()))\n",
    "print(t.flatten())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What I want you to notice about this output is that we have flattened the entire batch, and this smashes all the\n",
    "images together into a single axis. Remember the ones represent the pixels from the first image, the twos the second\n",
    "image, and the threes from the third.\n",
    "\n",
    "This flattened batch won’t work well inside our CNN because we need individual predictions for each image\n",
    "within our batch tensor, and now we have a flattened mess.\n",
    "\n",
    "The solution here, is to flatten each image while still maintaining the batch axis. This means we want to flatten\n",
    "only part of the tensor. We want to flatten the color channel axis with the height and width axes.\n",
    "These axes need to be flattened: (C,H,W)\n",
    "This axis cannot be flattened: N (batch size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flattening Specific Axes Of A Tensor\n",
    "Notice in the call how we specified the start_dim parameter. This tells the flatten() method which axis it should\n",
    "start the flatten operation. The one here is an index, so it’s the second axis which is the color channel axis.\n",
    "We skip over the batch axis so to speak, leaving it intact."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flattening: torch.Size([3, 1, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print('Before flattening:', t.shape)\n",
    "t = t.flatten(\n",
    "    start_dim=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking the shape, we can see that now we have a rank=2 tensor with three single color channel images that\n",
    "have been flattened out into 16 pixels.\n",
    "print('After flattening all except batch size dimension', t.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Flattening An RGB Image\n",
    "If we flatten an RGB image, what happens to the color ?\n",
    "\n",
    "Each color channel will be flattened first. Then, the flattened channels will be lined up side by side on a single axis of the tensor. Let's look at an example in code.\n",
    "We'll build an example RGB image tensor with a height of two and a width of two.\n",
    "\n",
    "create 3 individual images with single color channel=1, height=2 and width=2, (1, 2, 2) fulfilled with ones"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "r = torch.ones(1, 2, 2)\n",
    "g = torch.ones(1, 2, 2) + 1\n",
    "b = torch.ones(1, 2, 2) + 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "stack these individual images vertically (axis=0, by row), remember that axis is dim in PyTorch!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "img = torch.stack(\n",
    "    (r, g, b),\n",
    "    dim=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "finally we obtained image containing of 3 images, each with single color channel and having 2 x 2 pixel resolution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can see how this will look by flattening the image tensor, only by it's height and width."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.]],\n",
      "\n",
      "        [[2., 2., 2., 2.]],\n",
      "\n",
      "        [[3., 3., 3., 3.]]])\n"
     ]
    }
   ],
   "source": [
    "img = img.flatten(\n",
    "    start_dim=2,\n",
    "    end_dim=3\n",
    ")\n",
    "print(img)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Element-wise operations\n",
    "An element-wise operation operates on corresponding elements between tensors.\n",
    "\n",
    "Two elements are said to be corresponding if these two elements occupy the same position within the tensor.\n",
    "The position is determined by the indexes used to locate each element.\n",
    "\n",
    "Suppose we have the following two tensors:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corresponding elements (2nd row and 1st column): tensor(3.) tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor(\n",
    "    data=[\n",
    "        [1, 2],\n",
    "        [3, 4]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "t2 = torch.tensor(\n",
    "    data=[\n",
    "        [9, 8],\n",
    "        [7, 6]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "print('Corresponding elements (2nd row and 1st column):', t1[1][0], t2[1][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Two tensors must have the same shape in order to perform element-wise operations on them.\n",
    "\n",
    "Addition Is An Element-Wise Operation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10., 10.],\n",
      "        [10., 10.]])\n"
     ]
    }
   ],
   "source": [
    "print(t1 + t2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Arithmetic Operations Are Element-Wise Operations\n",
    "An operation we commonly see with tensors are arithmetic operations using scalar values.\n",
    "There are two ways we can do this:\n",
    "\n",
    "1) Using symbolic operations:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[18., 16.],\n",
      "        [14., 12.]])\n",
      "tensor([[0., 1.],\n",
      "        [1., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(t1 - 2)\n",
    "print(t2 * 2)\n",
    "print(t1 // 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2) using built-in tensor object methods:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[8., 6.],\n",
      "        [4., 2.]])\n",
      "tensor([[ 7., 14.],\n",
      "        [21., 28.]])\n",
      "tensor([[18., 16.],\n",
      "        [14., 12.]])\n"
     ]
    }
   ],
   "source": [
    "print(t1.add(2))\n",
    "print(t2.sub(t1))\n",
    "print(t1.mul(7))\n",
    "print(t2.div(0.5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Something seems to be wrong here. These examples are breaking the rule we established that said element-wise\n",
    "operations operate on tensors of the same shape.\n",
    "Scalar values are Rank-0 tensors, which means they have no shape, and our tensor t1 is a rank-2 tensor of shape 2 x 2.\n",
    "So how does this fit in? Let’s break it down.\n",
    "The first solution that may come to mind is that the operation is simply using the single scalar value and operating\n",
    "on each element within the tensor. This logic kind of works. However, it’s a bit misleading, and it breaks down in\n",
    "more general situations where we’re note using a scalar.\n",
    "To think about these operations differently, we need to introduce the concept of tensor broadcasting or broadcasting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Broadcasting Tensors\n",
    "Broadcasting describes how tensors with different shapes are treated during element-wise operations.\n",
    "Broadcasting is the concept whose implementation allows us to add scalars to higher dimensional tensors.\n",
    "\n",
    "Let's think about the t1 + 2 operation. Here, the scaler valued tensor is being broadcasted to the shape of t1,\n",
    "and then, the element-wise operation is carried out.\n",
    "\n",
    "We can see what the broadcasted scalar value looks like using the broadcast_to() Numpy function:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [2 2]]\n"
     ]
    }
   ],
   "source": [
    "print(np.broadcast_to(2, t1.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This means the scalar value is transformed into a rank-2 tensor just like t1, and just like that, the shapes match\n",
    "and the element-wise rule of having the same shape is back in play. This is all under the hood of course.\n",
    "\n",
    "Trickier Example Of Broadcasting"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(\n",
    "    size=(2, 2),\n",
    "    dtype=torch.float32\n",
    ")\n",
    "t2 = torch.tensor(\n",
    "    data=[4, 4],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "print(t1.shape)\n",
    "print(t2.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "t1 + t2 ????\n",
    "Even though these two tenors have differing shapes, the element-wise operation is possible, and broadcasting is what\n",
    "makes the operation possible. The lower rank tensor t2 will be transformed via broadcasting to match the shape of the\n",
    "higher rank tensor t1, and the element-wise operation will be performed as usual.\n",
    "\n",
    "The concept of broadcasting is the key to understanding how this operation will be carried out. As before,\n",
    "we can check the broadcast transformation using the broadcast_to() numpy function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4. 4.]\n",
      " [4. 4.]]\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "print(np.broadcast_to(t2.numpy(), t1.shape))\n",
    "print(t1 + t2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When do we actually use broadcasting? We often need to use broadcasting when we are preprocessing our data,\n",
    "and especially during normalization routines."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparison Operations Are Element-Wise\n",
    "Comparison operations are also element-wise operations.\n",
    "\n",
    "For a given comparison operation between two tensors, a new tensor of the same shape is returned with each element\n",
    "containing either a torch.bool value of True or False."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False],\n",
      "        [False,  True, False],\n",
      "        [False, False, False]])\n",
      "tensor([[False,  True,  True],\n",
      "        [ True,  True,  True],\n",
      "        [ True,  True,  True]])\n",
      "tensor([[ True, False, False],\n",
      "        [ True,  True,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([[False, False,  True],\n",
      "        [False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False],\n",
      "        [False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(\n",
    "    data=[\n",
    "        [1, 6, 9],\n",
    "        [4, 5, 2],\n",
    "        [3, 7, 2]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "print(t.eq(5))\n",
    "print(t.ge(2))\n",
    "print(t.le(5))\n",
    "print(t.gt(7))\n",
    "print(t.lt(1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Thinking about these operations from a broadcasting perspective, we can see that the last one, t.le(7),\n",
    "is really this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True, False],\n",
      "        [ True,  True,  True],\n",
      "        [ True,  True,  True]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    t <= torch.tensor(\n",
    "        np.broadcast_to(7, t.shape),\n",
    "        dtype=torch.float32\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Element-Wise Operations Using Functions\n",
    "With element-wise operations that are functions, it’s fine to assume that the function is applied to each element\n",
    "of the tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 6., 9.],\n",
      "        [4., 5., 2.],\n",
      "        [3., 7., 2.]])\n",
      "tensor([[1., 6., 9.],\n",
      "        [4., 5., 2.],\n",
      "        [3., 7., 2.]])\n",
      "tensor([[1.0000, 2.4495, 3.0000],\n",
      "        [2.0000, 2.2361, 1.4142],\n",
      "        [1.7321, 2.6458, 1.4142]])\n",
      "tensor([[-1., -6., -9.],\n",
      "        [-4., -5., -2.],\n",
      "        [-3., -7., -2.]])\n",
      "tensor([[1., 6., 9.],\n",
      "        [4., 5., 2.],\n",
      "        [3., 7., 2.]])\n"
     ]
    }
   ],
   "source": [
    "print(t)\n",
    "print(t.abs())\n",
    "print(t.sqrt())\n",
    "print(t.neg())\n",
    "print(t.neg().abs())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Some Terminology\n",
    "There are some other ways to refer to element-wise operations, all of these mean the same thing:\n",
    "\n",
    "    • Element-wise\n",
    "    • Component-wise\n",
    "    • Point-wise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Reduction Operations\n",
    "A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor.\n",
    "\n",
    "Tensors give us the ability to manage our data.\n",
    "\n",
    "Reshaping operations gave us the ability to position our elements along particular axes. Element-wise operations allow\n",
    "us to perform operations on elements between two tensors, and reduction operations allow us to perform operations on\n",
    "elements within a single tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "t = torch.tensor(\n",
    "    data = [\n",
    "        [0, 1, 0],\n",
    "        [2, 0, 2],\n",
    "        [0, 3, 0]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s look at our first reduction operation, a summation:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "print(t.sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the number of elements have been reduced by the operation, we can conclude that the sum() method is a\n",
    "reduction operation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Common Tensor Reduction Operations\n",
    "As you may expect, here are some other common reduction functions:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.8889)\n",
      "tensor(1.1667)\n",
      "tensor(3.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(t.prod())\n",
    "print(t.mean())\n",
    "print(t.std())\n",
    "print(t.max())\n",
    "print(t.min())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All of these tensor methods reduce the tensor to a single element scalar valued tensor by operating on all the\n",
    "tensor's elements.\n",
    "\n",
    "Reduction operations in general allow us to compute aggregate (total) values across data structures. In our case,\n",
    "our structures are tensors.\n",
    "\n",
    "Do reduction operations always reduce to a tensor with a single element? No!\n",
    "In fact, we often reduce specific axes at a time. This process is important. It’s just like we saw with reshaping\n",
    "when we aimed to flatten the image tensors within a batch while still maintaining the batch axis."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reducing Tensors By Axes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "t = torch.tensor(\n",
    "    data=[\n",
    "        [1, 1, 1, 1],\n",
    "        [2, 2, 2, 2],\n",
    "        [3, 3, 3, 3]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Sum using dim=0, vertical stacking"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    t.sum(dim=0)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Sum using dim=1, horizontal stacking"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.,  8., 12.])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    t.sum(dim=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Argmax Tensor Reduction Operation\n",
    "Argmax is a mathematical function that tells us which argument, when supplied to a function as input,\n",
    "results in the function’s max output value.\n",
    "\n",
    "Argmax returns the index location of the maximum value inside a tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "t = torch.tensor(\n",
    "    data=[\n",
    "        [1, 0, 0, 2],\n",
    "        [0, 3, 3, 0],\n",
    "        [4, 0, 0, 5]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tensor, we can see that the max value is the 5 in the last position of the last array."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "tensor(11)\n"
     ]
    }
   ],
   "source": [
    "print(t.max())\n",
    "print(t.argmax())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "he first piece of code confirms for us that the max is indeed 5, but the call to the argmax() method tells us that\n",
    "the 5 is sitting at index 11. What’s happening here?\n",
    "\n",
    "We’ll have a look at the flattened output for this tensor. If we don’t specific an axis to the argmax() method,\n",
    "it returns the index location of the max value from the flattened tensor, which in this case is indeed 11."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "print(t.flatten()[11])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see how we can work with specific axes now.\n",
    "\n",
    "Firstly max value using vertical stacking (finds max value in each column along the vertical axis)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([4., 3., 3., 5.]),\n",
      "indices=tensor([2, 1, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    t.max(dim=0)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now max values using horizontal stacking (finds max value in each row along the horizontal axis)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([2., 3., 5.]),\n",
      "indices=tensor([3, 1, 3]))\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    t.max(dim=1)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also find the indexes for max values using both vertical and horizontal stacking"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 1, 1, 2])\n",
      "tensor([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    t.argmax(dim=0)\n",
    ")\n",
    "print(t.argmax(dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each of these maximum values, the argmax() method tells us which element along the specified axis,\n",
    "where the value is located.\n",
    "\n",
    "In practice, we often use the argmax() function on a network’s output prediction tensor, to determine which\n",
    "category has the highest prediction value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accessing Elements Inside Tensors\n",
    "The last type of common operation that we need for tensors is the ability to access data from within the tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor(\n",
    "    data=[\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6],\n",
    "        [7, 8, 9]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "print(t.mean())\n",
    "print(t.mean().item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check out these operations on this one. When we call mean on this 3 x 3 tensor, the reduced output is a scalar valued\n",
    "tensor. If we want to actually get the value as a number, we use the item() tensor method.\n",
    "This works for scalar valued tensors.\n",
    "\n",
    "Have a look at how we do it with multiple values:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 5., 6.])\n",
      "[4.0, 5.0, 6.0]\n",
      "[2. 5. 8.]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    t.mean(dim=0)\n",
    ")\n",
    "print(\n",
    "    t.mean(dim=0).tolist()\n",
    ")\n",
    "print(\n",
    "    t.mean(dim=1).numpy()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Advanced Indexing And Slicing\n",
    "With NumPy ndarray objects, we have a pretty robust set of operations for indexing and slicing, and PyTorch tensor\n",
    "objects support most of these operations as well."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}