{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building The Model Is Next\n",
    "To build neural networks in PyTorch, we extend the torch.nn.Module PyTorch class. This means we need to utilize a\n",
    "little bit of object oriented programming (OOP) in Python.\n",
    "\n",
    "To build a convolutional neural network, we need to have a general understanding of how CNNs work and what components\n",
    "are used to build CNNs:\n",
    "\n",
    "    · Convolutional Filters\n",
    "    · Zero Padding\n",
    "    · Max Pooling\n",
    "    · Learnable Parameters in CNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch’s torch.nn Package\n",
    "To build neural networks in PyTorch, we use the torch.nn package, which is PyTorch’s neural network (nn) library.\n",
    "We typically import the package like so:\n",
    "\n",
    "    · import torch.nn as nn\n",
    "\n",
    "PyTorch’s neural network library contains all of the typical components needed to build neural networks.\n",
    "\n",
    "The primary component we'll need to build a neural network is a layer, and so, as we might expect, PyTorch's neural\n",
    "network library contains classes that aid us in constructing layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch's nn.Module Class\n",
    "As we know, deep neural networks are built using multiple layers. This is what makes the network deep. Each layer in a\n",
    "neural network has two primary components:\n",
    "\n",
    "    · A transformation (code)\n",
    "    · A collection of weights (data)\n",
    "\n",
    "Like many things in life, this fact makes layers great candidates to be represented as objects using OOP.\n",
    "\n",
    "Within the nn package, there is a class called Module, and it is the base class for all of neural network modules which\n",
    "includes layers.\n",
    "\n",
    "This means that all of the layers in PyTorch extend the nn.Module class and inherit all of PyTorch’s built-in\n",
    "functionality within the nn.Module class. In OOP this concept is known as inheritance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch nn.Modules Have A forward() Method\n",
    "When we pass a tensor to our network as input, the tensor flows forward though each layer transformation until the\n",
    "tensor reaches the output layer. This process of a tensor flowing forward though the network is known as a forward pass\n",
    "or forward propagation.\n",
    "\n",
    "The tensor input is passed forward though the network.\n",
    "\n",
    "The goal of the overall transformation is to transform or map the input to the correct prediction output class, and\n",
    "during the training process, the layer weights (data) are updated in such a way that cause the mapping to adjust to\n",
    "make the output closer to the correct prediction.\n",
    "\n",
    "What this all means is that, every PyTorch nn.Module has a forward() method, and so when we are building layers and\n",
    "networks, we must provide an implementation of the forward() method. The forward method is the actual transformation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch’s nn.functional Package\n",
    "When we implement the forward() method of our nn.Module subclass, we will typically use functions from the nn.functional\n",
    "package. This package provides us with many neural network operations that we can use for building layers. In fact,\n",
    "many of the nn.Module layer classes use nn.functional functions to perform their operations.\n",
    "\n",
    "The nn.functional package contains methods that subclasses of nn.Module use for implementing their forward() functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building A Neural Network In PyTorch\n",
    "We now have enough information to provide an outline for building neural networks in PyTorch. The steps are as follows:\n",
    "\n",
    "Short version:\n",
    "\n",
    "    1.  Extend the nn.Module base class.\n",
    "    2.  Define layers as class attributes.\n",
    "    3.  Implement the forward() method.\n",
    "\n",
    "More detailed version:\n",
    "\n",
    "    1.   Create a neural network class that extends the nn.Module base class.\n",
    "    2.  In the class constructor, define the network’s layers as class attributes using pre-built layers from torch.nn.\n",
    "    3.  Use the network’s layer attributes as well as operations from the nn.functional API to define the network’s\n",
    "    forward pass."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extending PyTorch’s nn.Module Class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class DummyNetwork:\n",
    "    def __init__(self):\n",
    "        self.layer = None,\n",
    "    def forward(self, tensor):\n",
    "        new_tensor = self.layer(tensor)\n",
    "        return new_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This gives us a simple network class that has a single dummy layer inside the constructor and a dummy implementation for\n",
    " the forward function.\n",
    "\n",
    "The implementation for the forward() function takes in a tensor t and transforms it using the dummy layer. After the\n",
    "tensor is transformed, the new tensor is returned.\n",
    "\n",
    "This is a good start, but the class hasn’t yet extended the nn.Module class. To make our Network class extend nn.Module,\n",
    "we must do two additional things:\n",
    "\n",
    "    1.  Specify the nn.Module class in parentheses on line 1.\n",
    "    2.  Insert a call to the super class constructor on line 3 inside the constructor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # super class call . constructor from super class call\n",
    "        self.layer = None\n",
    "    def forward(self, t):\n",
    "        new_t = self.layer(t)\n",
    "        return new_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These changes transform our simple neural network into a PyTorch neural network because we are now extending PyTorch's\n",
    "nn.Module base class.\n",
    "\n",
    "With this, we are done! Now we have a Network class that has all of the functionality of the PyTorch nn.Module class."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define The Network’s Layers As Class Attributes\n",
    "At the moment, our Network class has a single dummy layer as an attribute. Let’s replace this now with some real layers\n",
    "that come pre-built for us from PyTorch's nn library. We’re building a CNN, so the two types of layers we'll use are\n",
    "linear (dense) layers and convolutional layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class BaseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # call the constructor from super class!\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=6,\n",
    "            kernel_size=5\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=6,\n",
    "            out_channels=12,\n",
    "            kernel_size=5\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=12 * 4 * 4,\n",
    "            out_features=120\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=120,\n",
    "            out_features=60\n",
    "        )\n",
    "        self.out = nn.Linear(\n",
    "            in_features=60,\n",
    "            out_features=10\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # implement the forward pass\n",
    "        return t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alright. At this point, we have a Python class called Network that extends PyTorch’s nn.Module class. Inside of our\n",
    "Network class, we have five layers that are defined as attributes. We have two convolutional layers, self.conv1 and\n",
    "self.conv2, and three linear layers, self.fc1, self.fc2, self.out.\n",
    "\n",
    "We used the abbreviation fc in fc1 and fc2 because linear layers are also called fully connected layers. They also have\n",
    "a third name that we may hear sometimes called dense. So linear, dense, and fully connected are all ways to refer to the\n",
    "same type of layer. PyTorch uses the word linear, hence the nn.Linear class name.\n",
    "\n",
    "We used the name out for the last linear layer because the last layer in the network is the output layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch CNN Layer Parameters\n",
    "Parameter vs Argument:\n",
    "\n",
    "    · parameters are used in function definitions as place-holders, while arguments are the actual values/objects that\n",
    "    are passed to the function.\n",
    "    · parameters can be thought of as local variables that live inside a function\n",
    "\n",
    "Two Types Of Parameters:\n",
    "\n",
    "    1.  Hyperparameters\n",
    "    2.  Data dependent hyperparameters\n",
    "\n",
    "The main thing to remember about any type of parameter is that the parameter is a place-holder that will eventually\n",
    "hold or have a value (of the object that is passed in a function)\n",
    "\n",
    "When we construct a layer, we pass values for each parameter to the layer’s constructor. With our convolutional layers\n",
    "have three parameters and the linear layers have two parameters.\n",
    "\n",
    "    1.  Convolutional layers\n",
    "        · in_channels\n",
    "        · out_channels\n",
    "        · kernel_size\n",
    "    2.  Linear layers\n",
    "        · in_features\n",
    "        · out_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hyperparameters\n",
    "In general, hyperparameters are parameters whose values are chosen manually and arbitrarily.\n",
    "\n",
    "As neural network programmers, we choose hyperparameter values mainly based on trial and error and increasingly by\n",
    "utilizing values that have proven to work well in the past. For building our CNN layers, these are the parameters we\n",
    "choose manually:\n",
    "\n",
    "        · kernel_size (Conv)\n",
    "        · out_channels (Conv)\n",
    "        · out_features (Linear)\n",
    "\n",
    "This means we simply choose the values for these parameters. In neural network programming, this is pretty common, and\n",
    "we usually test and tune these parameters to find values that work best.\n",
    "\n",
    "\n",
    "Parameter\tDescription\n",
    "\n",
    "    · kernel_size\t Sets the filter size. The words kernel and filter are interchangeable.\n",
    "    · out_channels\tSets the number of filters. One filter produces one output channel.\n",
    "    · out_features\tSets the size of the output tensor.\n",
    "\n",
    "On pattern that shows up quite often is that we increase our out_channels as we add additional conv layers, and after we\n",
    "switch to linear layers we shrink our out_features as we filter down to our number of output classes.\n",
    "\n",
    "All of these parameters impact our network's architecture. Specifically, these parameters directly impact the weight\n",
    "tensors inside the layers.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Dependent Hyperparameters\n",
    "Data dependent hyperparameters are parameters whose values are dependent on data. The first two data dependent\n",
    "hyperparameters that stick out are the in_channels of the first convolutional layer, and the out_features of the output\n",
    "layer.\n",
    "\n",
    "You see, the in_channels of the first convolutional layer depend on the number of color channels present inside the\n",
    "images that make up the training set. Since we are dealing with grayscale images, we know that this value should be a 1.\n",
    "\n",
    "The out_features for the output layer depend on the number of classes that are present inside our training set. Since\n",
    "we have 10 classes of clothing inside the Fashion-MNIST dataset, we know that we need 10 output features.\n",
    "\n",
    "In general, the input to one layer is the output from the previous layer, and so all of the in_channels in the conv\n",
    "layers and in_features in the linear layers depend on the data coming from the previous layer.\n",
    "\n",
    "When we switch from a conv layer to a linear layer, we have to flatten our tensor. This is why we have 12*4*4.\n",
    "The twelve comes from the number of output channels in the previous layer, but why do we have the two 4s?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary Of Layer Parameters\n",
    "    Layer\tParam name\tParam value\tThe param value is\n",
    "\n",
    "    · conv1\tin_channels\t1\tthe number of color channels in the input image (data dependent hyperparameter).\n",
    "    · conv1\tkernel_size\t5\ta hyperparameter.\n",
    "    · conv1\tout_channels\t6\ta hyperparameter.\n",
    "    · conv2\tin_channels\t6\tthe number of out_channels in previous layer.\n",
    "    · conv2\tkernel_size\t5\ta hyperparameter.\n",
    "    · conv2\tout_channels\t12\ta hyperparameter (higher than previous conv layer).\n",
    "    · fc1\tin_features\t12*4*4\tthe length of the flattened output from previous layer.\n",
    "    · fc1\tout_features\t120\ta hyperparameter.\n",
    "    · fc2\tin_features\t120\tthe number of out_features of previous layer.\n",
    "    · fc2\tout_features\t60\ta hyperparameter (lower than previous linear layer).\n",
    "    · out\tin_features\t60\tthe number of out_channels in previous layer.\n",
    "    · out\tout_features\t10\tthe number of prediction classes (data dependent hyperparameter)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learnable Parameters\n",
    "Learnable parameters are parameters whose values are learned during the training process.\n",
    "\n",
    "With learnable parameters, we typically start out with a set of arbitrary values, and these values then get updated in\n",
    "an iterative fashion as the network learns.\n",
    "\n",
    "In fact, when we say that a network is learning, we specifically mean that the network is learning the appropriate\n",
    "values for the learnable parameters. Appropriate values are values that minimize the loss function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting An Instance The Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseNetwork(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "network = BaseNetwork()\n",
    "print(network)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The print() function prints to the console a string representation of our network. With a sharp eye, we can notice that\n",
    "the printed output here is detailing our network’s architecture listing out our network’s layers, and showing the values\n",
    "that were passed to the layer constructors."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network String Representation\n",
    "We can override Python’s default string representation using the __repr__ function. This name is short for\n",
    "representation.\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"lizardnet\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Layers\n",
    "For the convolutional layers, the kernel_size argument is a Python tuple (5,5) even though we only passed the number 5\n",
    "in the constructor.\n",
    "\n",
    "    print(network)\n",
    "    >>\n",
    "        Network(\n",
    "        (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
    "        (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
    "        (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
    "        (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
    "        (out): Linear(in_features=60, out_features=10, bias=True)\n",
    "    )\n",
    "\n",
    "This is because our filters actually have a height and width, and when we pass a single number, the code inside the\n",
    "layer’s constructor assumes that we want a square filter.\n",
    "\n",
    "\n",
    "The stride is an additional parameter that we could have set, but we left it out. When the stride is not specified in\n",
    "the layer constructor, the layer automatically sets it to stride=(1, 1).\n",
    "\n",
    "The stride tells the conv layer how far the filter should slide after each operation in the overall convolution. This\n",
    "tuple says to slide by one unit when moving to the right and also by one unit when moving down."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Layers\n",
    "For the linear layers, we have an additional parameter called bias which has a default parameter value of true. It is\n",
    "possible to turn this off by setting it to false."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accessing The Network's Layers\n",
    "In Python and many other programming languages, we access attributes and methods of objects using dot notation.\n",
    "\n",
    "Something to notice about this that pertains directly to what we were just talking about with the string representation\n",
    "of the network is that each of these pieces of code are also giving us a string representation of each layer.\n",
    "\n",
    "In the network’s case, the network class is really just compiling all this data together to give us a single output."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "Linear(in_features=192, out_features=120, bias=True)\n",
      "Linear(in_features=60, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(network.conv2)\n",
    "print(network.fc1)\n",
    "print(network.out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accessing The Layer Weights\n",
    "Now that we have access to each of our layers, we can access the weights inside each layer. Let’s see this for our first\n",
    "convolutional layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.1286, -0.0964, -0.0938, -0.0088,  0.1372],\n",
      "          [ 0.1176,  0.1968,  0.1176, -0.0493,  0.0252],\n",
      "          [-0.0985, -0.0360,  0.1453,  0.0308,  0.0313],\n",
      "          [ 0.0316, -0.0961,  0.1433,  0.0123, -0.1044],\n",
      "          [-0.0978,  0.0483,  0.0139,  0.0330,  0.1033]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0438, -0.0298, -0.0539,  0.1358, -0.0938],\n",
      "          [-0.0501, -0.1116, -0.0380,  0.0185, -0.1113],\n",
      "          [-0.1996, -0.0043,  0.1479,  0.0139,  0.0021],\n",
      "          [-0.0104, -0.1027,  0.1406,  0.0932,  0.0439],\n",
      "          [ 0.0424, -0.1627,  0.1499, -0.1887,  0.1181]]],\n",
      "\n",
      "\n",
      "        [[[-0.1045, -0.0645,  0.0957,  0.1842,  0.0730],\n",
      "          [ 0.0142,  0.0451, -0.1735, -0.1145,  0.0269],\n",
      "          [-0.0070, -0.0168, -0.0029,  0.0079,  0.1247],\n",
      "          [ 0.1884,  0.0991, -0.1161, -0.0263, -0.0984],\n",
      "          [-0.1861,  0.1013, -0.1887,  0.0015, -0.0855]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1077, -0.1548, -0.0075,  0.0654,  0.1730],\n",
      "          [-0.1209,  0.1122, -0.1333, -0.0906, -0.0489],\n",
      "          [-0.0661,  0.1942, -0.0429, -0.0068, -0.0126],\n",
      "          [ 0.0148,  0.0849,  0.0607,  0.0130,  0.0799],\n",
      "          [-0.0355, -0.1948, -0.0823,  0.0437, -0.1700]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0186,  0.1964, -0.1451, -0.1252, -0.0133],\n",
      "          [ 0.1176,  0.0019, -0.0930,  0.0217,  0.1760],\n",
      "          [ 0.1716, -0.0158,  0.1056, -0.1667, -0.0876],\n",
      "          [ 0.1638,  0.1601, -0.1003,  0.1801, -0.1848],\n",
      "          [ 0.0059, -0.1657, -0.1162, -0.1506, -0.0828]]],\n",
      "\n",
      "\n",
      "        [[[-0.0855,  0.0554, -0.0375,  0.1009, -0.1882],\n",
      "          [ 0.1767,  0.1681,  0.0004,  0.0646, -0.0704],\n",
      "          [ 0.0564,  0.1254,  0.1577, -0.0461,  0.1976],\n",
      "          [-0.1438,  0.1703, -0.1215, -0.0157, -0.0143],\n",
      "          [-0.1502,  0.0805, -0.1671,  0.0442, -0.0904]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    network.conv1.weight\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One thing to notice about the weight tensor output is that it says parameter containing at the top of the output. This\n",
    "is because this particular tensor is a special tensor because its values or scalar components are learnable parameters\n",
    "of our network.\n",
    "\n",
    "This means that the values inside this tensor, the ones we see above, are actually learned as the network is trained.\n",
    "As we train, these weight values are updated in such a way that the loss function is minimized."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PyTorch Parameter Class\n",
    "To keep track of all the weight tensors inside the network. PyTorch has a special class called Parameter. The Parameter\n",
    "class extends the tensor class, and so the weight tensor inside every layer is an instance of this Parameter class.\n",
    "This is why we see the Parameter containing text at the top of the string representation output.\n",
    "\n",
    "PyTorch’s nn.Module class is basically looking for any attributes whose values are instances of the Parameter class,\n",
    "and when it finds an instance of the parameter class, it keeps track of it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weight Tensor Shape\n",
    "The parameter values we pass to our layers directly impact our network’s weights.\n",
    "\n",
    "For the convolutional layers, the weight values live inside the filters, and in code, the filters are actually the\n",
    "weight tensors themselves.\n",
    "\n",
    "The convolution operation inside a layer is an operation between the input channels to the layer and the filter inside\n",
    "the layer. This means that what we really have is an operation between two tensors.\n",
    "\n",
    "For the first conv layer, we have 1 color channel that should be convolved by 6 filters of size 5x5 to produce 6 output\n",
    "channels. This is how we interpret the values inside our layer constructor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inside our layer though, we don’t explicitly have 6 weight tensors for each of the 6 filters.\n",
    "We actually represent all 6 filters using a single weight tensor whose shape reflects or accounts for the 6 filters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    network.conv1.weight.shape\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second axis has a length of 1 which accounts for the single input channel, and the last two axes account for the\n",
    "height and width of the filter.\n",
    "\n",
    "The way to think about this is as if we are packaging all of our filters into a single tensor.\n",
    "\n",
    "Now, the second conv layer has 12 filters, and instead of convolving a single input channel, there are 6 input channels\n",
    "coming from the previous layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 6, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    network.conv2.weight.shape\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Think of this value of 6 here as giving each of the filters some depth. Instead of having a filter that convolves all of\n",
    "the channels iteratively, our filter has a depth that matches the number of channels.\n",
    "\n",
    "The two main takeaways about these convolutional layers is that our filters are represented using a single tensor and\n",
    "that each filter inside the tensor also has a depth that accounts for the input channels that are being convolved.\n",
    "\n",
    "    1.  All filters are represented using a single tensor.\n",
    "    2.  Filters have depth that accounts for the input channels.\n",
    "\n",
    "Our tensors are rank-4 tensors. The first axis represents the number of filters. The second axis represents the depth of\n",
    "each filter which corresponds to the number of input channels being convolved.\n",
    "\n",
    "The last two axes represent the height and width of each filter. We can pull out any single filter by indexing into the\n",
    "weight tensor’s first axis.\n",
    "\n",
    "    Weight tensor = [\n",
    "        Number of filters (output_channel),\n",
    "            Depth (input_channel),\n",
    "                Height,\n",
    "                     Width (kernel_size)\n",
    "                     ]\n",
    "\n",
    "This gives us a single filter that has a height and width of 5 and a depth of 6."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Weight Matrix\n",
    "With linear layers or fully connected layers, we have flattened rank-1 tensors as input and as output. The way we\n",
    "transform the in_features to the out_features in a linear layer is by using a rank-2 tensor that is commonly called a\n",
    "weight matrix.\n",
    "\n",
    "This is due to the fact that the weight tensor is of rank-2 with height and width axes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 192])\n",
      "torch.Size([60, 120])\n",
      "torch.Size([10, 60])\n"
     ]
    }
   ],
   "source": [
    "print(network.fc1.weight.shape)\n",
    "print(network.fc2.weight.shape)\n",
    "print(network.out.weight.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see that each of our linear layers have a rank-2 weight tensor. The pattern that we can see here is that the\n",
    "height of the weight tensor has the length of the desired output features and a width of the input features - the first\n",
    "axis represents the out_features, and second axis represents the in_features. out_features from conv(n-1) layer are\n",
    "in_features to conv(n) layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Function Represented Using A Matrix\n",
    "The important thing about matrix multiplications like this is that they represent linear functions that we can use to\n",
    "build up our neural network.\n",
    "\n",
    "Specifically, the weight matrix is a linear function also called a linear map that maps a vector space of 4 dimensions\n",
    "to a vector space of 3 dimensions.\n",
    "\n",
    "When we change the weight values inside the matrix, we are actually changing this function, and this is exactly what we\n",
    "want to do as we search for the function that our network is ultimately approximating."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using PyTorch For Matrix Multiplication\n",
    "Here, we have the in_features and the weight_matrix as tensors, and we’re using the tensor method called matmul() to\n",
    "perform the operation. The name matmul() as we now know is short for matrix multiplication.\n",
    "\n",
    "    weight_matrix.matmul(in_features)\n",
    "        >> tensor([30., 40., 50.])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Accessing The Networks Parameters\n",
    "The first example is the most common way, and we’ll use this to iterate over our weights when we update them during the\n",
    "training process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([12, 6, 5, 5])\n",
      "torch.Size([12])\n",
      "torch.Size([120, 192])\n",
      "torch.Size([120])\n",
      "torch.Size([60, 120])\n",
      "torch.Size([60])\n",
      "torch.Size([10, 60])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in network.parameters():\n",
    "    print(param.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second way is just to show how we can see the name as well. This reveals something that we won’t cover in detail,\n",
    "the bias is also a learnable parameter. Each layer has a bias by default, so for each layer we have a weight tensor and\n",
    "a bias tensor."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t\t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t\t torch.Size([6])\n",
      "conv2.weight \t\t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t\t torch.Size([12])\n",
      "fc1.weight \t\t torch.Size([120, 192])\n",
      "fc1.bias \t\t torch.Size([120])\n",
      "fc2.weight \t\t torch.Size([60, 120])\n",
      "fc2.bias \t\t torch.Size([60])\n",
      "out.weight \t\t torch.Size([10, 60])\n",
      "out.bias \t\t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, '\\t\\t', param.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How Linear Layers Work\n",
    "When the input features are received by a linear layer, they are received in the form of a flattened 1-dimensional\n",
    "tensor and are then multiplied by the weight matrix. This matrix multiplication produces the output features."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transform Using A Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30., 40., 50.])\n"
     ]
    }
   ],
   "source": [
    "temp_in_features = torch.tensor(\n",
    "    data=[1, 2, 3, 4],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "temp_weight_matrix = torch.tensor(\n",
    "    data=[\n",
    "        [1, 2, 3, 4],\n",
    "        [2, 3, 4, 5],\n",
    "        [3, 4, 5, 6]\n",
    "    ],\n",
    "    dtype=torch.float32\n",
    ")\n",
    "\n",
    "print(\n",
    "    temp_weight_matrix.matmul(temp_in_features)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we have created a 1-dimensional tensor called in_features. We have also created a weight matrix which of course is\n",
    "a 2-dimensional tensor. Then, we've use the matmul() function to preform the matrix multiplication operation that\n",
    "produces a 1-dimensional tensor.\n",
    "\n",
    "In general, the weight matrix defines a linear function that maps a 1-dimensional tensor with four elements to a\n",
    "1-dimensional tensor that has three elements. We can think of this function as a mapping from 4-dimensional Euclidean\n",
    "space to 3-dimensional Euclidean space.\n",
    "\n",
    "This is how linear layers work as well. They map an in_feature space to an out_feature space using a weight matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transform Using A PyTorch Linear Layer\n",
    "Let's see how to create a PyTorch linear layer that will do this same operation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "temp_fc = nn.Linear(\n",
    "    in_features=4,\n",
    "    out_features=3,\n",
    "    bias=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here, we have it. We've defined a linear layer that accepts 4 in features and transforms these into 3 out features, so\n",
    "we go from 4-dimensional space to 3-dimensional space. We know that a weight matrix is used to preform this operation,\n",
    "but where is the weight matrix in this example?\n",
    "\n",
    "We'll the weight matrix is lives inside the PyTorch LinearLayer class and is created by PyTorch. The PyTorch LinearLayer\n",
    "class uses the numbers 4 and 3 that are passed to the constructor to create a 3 x 4 weight matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4422, -1.2583,  0.6285], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "print(temp_fc(temp_in_features))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch creates a weight matrix and initializes it with random values. This means that the linear functions from the two\n",
    "examples are different, so we are using different function to produce these outputs.\n",
    "\n",
    "Remember the values inside the weight matrix define the linear function. This demonstrates how the network's mapping\n",
    "changes as the weights are updated during the training process.\n",
    "\n",
    "Let's explicitly set the weight matrix of the linear layer to be the same as the one we used in our other example."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "temp_fc.weight = nn.Parameter(temp_weight_matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch module weights need to be parameters. This is why we wrap the weight matrix tensor inside a parameter class\n",
    "instance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "print(temp_fc(temp_in_features))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The output will not be exact because the linear layer is adding a bias tensor to the output. Watch what happens when we\n",
    "turn the bias off. We do this by passing a False flag to the constructor.\n",
    "\n",
    "The results are now the same for both linear functions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mathematical Notation Of The Linear Transformation\n",
    "Sometimes we'll see linear layer operation referred to as\n",
    "\n",
    "    y = Ax + b\n",
    "\n",
    "    Variable\tDefinition\n",
    "    · A  Weight matrix tensor\n",
    "    · x  Input tensor\n",
    "    · b  Bias tensor\n",
    "    · y  Output tensor\n",
    "\n",
    "We'll note that this is similar to the equation for a line y = mx + b."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Callable Layers And Neural Networks\n",
    "We pointed out before how it was kind of strange that we called the layer object instance as if it were a function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    temp_fc(temp_in_features)\n",
    "    # to see the output, simply call the object instance as a function, passing in_features as parameter.async\n",
    "    # the output is a result of multiplication of the flattend 1d input tensor and the weight matrix.\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What makes this possible is that PyTorch module classes implement another special Python function called __call__().\n",
    "If a class implements the __call__() method, the special call method will be invoked anytime the object instance is\n",
    "called.\n",
    "\n",
    "This fact is an important PyTorch concept because of the way the __call__() method interacts with the forward() method for our layers and networks.\n",
    "\n",
    "Instead of calling the forward() method directly, we call the object instance. After the object instance is called, the\n",
    "__call__() method is invoked under the hood, and the __call__() in turn invokes the forward() method. This applies to\n",
    "all PyTorch neural network modules, namely, networks and layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN Forward Pass Implementation\n",
    "Use the network's layer attributes as well nn.functional API operations to define the network's forward pass."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Implementing The forward() Method\n",
    "Input Layer #1\n",
    "\n",
    "    The input layer of any neural network is determined by the input data. For example, if our input tensor contains\n",
    "    three elements, our network would have three nodes contained in its input layer.\n",
    "\n",
    "    For this reason, we can think of the input layer as the identity transformation.\n",
    "    Mathematically, this is the function f(x) = x\n",
    "\n",
    "    We give any x as the input, and we get back the same x as the output. This logic is the same regardless of whether\n",
    "    we're working with a tensor that has three elements, or a tensor that represents an image with three channels.\n",
    "    The in is the data out!\n",
    "\n",
    "Hidden Convolutional Layers: Layers #2 And #3\n",
    "\n",
    "    Both of the hidden convolutional layers are going to be very similar in terms of performing the transformation.\n",
    "    all layers that are not the input or output layers are called hidden layers, and this is why we are referring to\n",
    "    these convolutional layers as hidden layers.\n",
    "\n",
    "    To preform the convolution operation, we pass the tensor to the forward method of the first convolutional layer,\n",
    "    self.conv1. We've learned how all PyTorch neural network modules have forward() methods, and when we call the\n",
    "    forward() method of a nn.Module, there is a special way that we make the call.\n",
    "\n",
    "    When want to call the forward() method of a nn.Module instance, we call the actual instance instead of calling the\n",
    "    forward() method directly.\n",
    "\n",
    "    Instead of doing this self.conv1.forward(tensor), we do this self.conv1(tensor).\n",
    "\n",
    "Let's go ahead and add all the calls needed to implement both of our convolutional layers.\n",
    "\n",
    "        # Hidden conv layer conv1 (conv1 is the first hidden layer!)\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # Hidden conv layer conv2\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    As we can see here, our input tensor is transformed as we move through the convolutional layers. The first\n",
    "    convolutional layer has a convolutional operation, followed by a relu activation operation whose output is then passed\n",
    "    to a max pooling operation with kernel_size=2 and stride=2.\n",
    "\n",
    "    The output tensor t of the first convolutional layer is then passed to the next convolutional layer, which is identical\n",
    "    except for the fact that we call self.conv2() instead of self.conv1().\n",
    "\n",
    "    Each of these layers is comprised of a collection of weights (data) and a collection of operations (code). The weights\n",
    "    are encapsulated inside the nn.Conv2d() class instance. The relu() and the max_pool2d() calls are just pure operations.\n",
    "    Neither of these have weights, and this is why we call them directly from the nn.functional API.\n",
    "\n",
    "    Sometimes we may see pooling operations referred to as pooling layers. Sometimes we may even hear activation operations\n",
    "    called activation layers (Tensorflow, Keras).\n",
    "\n",
    "    However, what makes a layer distinct from an operation is that layers have weights. Since pooling operations and\n",
    "    activation functions do not have weights, we will refer to them as operations and view them as being added to the\n",
    "    collection of layer operations.\n",
    "\n",
    "    Mathematically, the entire network is just a composition of functions, and a composition of functions is a function\n",
    "    itself. So a network is just a function. All the terms like layers, activation functions, and weights, are just used to\n",
    "    help describe the different parts.\n",
    "\n",
    "Hidden Linear Layers: Layers #4 And #5\n",
    "\n",
    "    Before we pass our input to the first hidden linear layer, we must reshape() or flatten our tensor. This will be the\n",
    "    case any time we are passing output from a convolutional layer as input to a linear layer.\n",
    "\n",
    "            # Hidden linear layer fc1 (connection between conv and linear layers needs reshaping!)\n",
    "            t = t.reshape(-1, 12 * 4 * 4)\n",
    "            t = self.fc1(t)\n",
    "            t = F.relu(t)\n",
    "\n",
    "            # Hidden linear layer fc2\n",
    "            t = self.fc2(t)\n",
    "            t = F.relu(t)\n",
    "\n",
    "    We saw in the post on CNN weights that the number 12 in the reshaping operation is determined by the number of\n",
    "    output channels coming from the previous convolutional layer.\n",
    "\n",
    "    The 4 * 4 is actually the height and width of each of the 12 output channels.\n",
    "\n",
    "    We started with a 1 x 28 x 28 input tensor. This gives a single color channel, 28 x 28 image, and by the time our\n",
    "    tensor arrives at the first linear layer, the dimensions have changed.\n",
    "\n",
    "    The height and width dimensions have been reduced from 28 x 28 to 4 x 4 by the convolution and pooling operations.\n",
    "\n",
    "    Convolution and pooling operations are reduction operations on the height and width dimensions.\n",
    "\n",
    "\n",
    "Output Layer #6\n",
    "\n",
    "    The sixth and last layer of our network is a linear layer we call the output layer. When we pass our tensor to the\n",
    "    output layer, the result will be the prediction tensor. Since our data has ten prediction classes, we know our\n",
    "    output tensor will have ten elements.\n",
    "\n",
    "        # Output linear layer\n",
    "        t = self.out(t)\n",
    "        # t = F.softmax(t, dim=1)\n",
    "        # we will use F.cross_entropy() loss function, which implicitly performs softmax operation on its inputm\n",
    "\n",
    "    The values inside each of the ten components will correspond to the prediction value for each of our prediction\n",
    "    classes.\n",
    "\n",
    "    Inside the network we usually use relu() as our non-linear activation function, but for the output layer, whenever\n",
    "    we have a single category that we are trying to predict, we use softmax(). The softmax function returns a positive\n",
    "    probability for each of the prediction classes, and the probabilities sum to 1.\n",
    "\n",
    "    However, in our case, we won't use softmax() because the loss function that we'll use, F.cross_entropy(), implicitly\n",
    "    performs the softmax() operation on its input, so we'll just return the result of the last linear transformation.\n",
    "\n",
    "    The implication of this is that our network will be trained using the softmax operation but will not need to compute the\n",
    "    additional operation when the network is used for inference after the training process is complete.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=6,\n",
    "            kernel_size=5,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=6,\n",
    "            out_channels=12,\n",
    "            kernel_size=5,\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=12 * 4 * 4,\n",
    "            out_features=120,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=120,\n",
    "            out_features=60,\n",
    "        )\n",
    "        self.out = nn.Linear(\n",
    "            in_features=60,\n",
    "            out_features=10\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        # First Convolutional Hidden Layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # Second Convolutional Hidden Layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # First Linear Layer (needs reshaping!)\n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        # reshape the tensor, so it has 12 * 4 * 4 input features (axis=1) and corresponding number of outputs (axis=0)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # Second Linear Layer (does not need reshaping, it was reshaped during the transition between conv to linear)\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # Output layer\n",
    "        t = self.out(t)\n",
    "        # t = F.softmax(t, dim=1) # dim=1 as input because dim=0 is the output.\n",
    "        # we do not use the softmax operation there, because our loss function cross_entropy uses it for us.\n",
    "\n",
    "        return t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What Is Forward Propagation?\n",
    "Forward propagation is the process of transforming an input tensor to an output tensor. At its core, a neural network is\n",
    "a function that maps an input tensor to an output tensor, and forward propagation is just a special name for the process\n",
    "of passing an input to the network and receiving the output from the network.\n",
    "\n",
    "As we have seen, neural networks operate on data in the form of tensors. The concept forward propagate is used to\n",
    "indicate that the input tensor data is transmitted through the network in the forward direction.\n",
    "\n",
    "For our network, what this means is simply passing our input tensor to the network and receiving the output tensor. To\n",
    "do this, we pass our sample data to the network's forward() method.\n",
    "\n",
    "This is why, the forward() method has the name forward, the execution of the forward() is the process of forward\n",
    "propagation.\n",
    "\n",
    "However, the word propagate means to move or transmit through some medium. In the case of neural networks, data propagates through the layers of the network.\n",
    "\n",
    "There is a notion of backward propagation (backpropagation) as well which makes the term forward propagation suitable as\n",
    "a first step. During the training process, backpropagation occurs after forward propagation.\n",
    "\n",
    "The computation graph keeps track of the network's mapping by tracking each computation that happens. The graph is used\n",
    "during the training process to calculate the derivative (gradient) of the loss function with respect to the network’s\n",
    "weights.\n",
    "\n",
    "This process of tracking calculations happens in real-time, as the calculations occur. PyTorch uses a dynamic\n",
    "computational graph."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Passing A Single Image To The Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=10,\n",
    "    shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sample is a first element of iteration on train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "sample = next(\n",
    "    iter(train_set)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "while Batch is a first element of iteration on data_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:  Image: torch.Size([1, 28, 28]), Label: torch.Size([])\n",
      "Batch:   Images: torch.Size([10, 1, 28, 28]), Labels: torch.Size([10])\n",
      "First image:  torch.Size([1, 28, 28]) tensor(2)\n"
     ]
    }
   ],
   "source": [
    "batch = next(\n",
    "    iter(data_loader)\n",
    ")\n",
    "\n",
    "single_image, single_label = sample\n",
    "images, labels = batch\n",
    "\n",
    "print('Sample:  Image: {}, Label: {}'.format(single_image.shape, torch.tensor(single_label).shape))\n",
    "print('Batch:   Images: {}, Labels: {}'.format(images.shape, labels.shape))\n",
    "\n",
    "print('First image: ', images[0].shape, labels[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we pass a tensor to our network, the network is expecting a batch, so even if we want to pass a single image, we\n",
    "still need a batch.\n",
    "\n",
    "This is no problem. We can create a batch that contains a single image. All of this will be packaged into a single four\n",
    "dimensional tensor that reflects the following dimensions.\n",
    "\n",
    "       · (batch_size, in_channels, height, width)\n",
    "\n",
    "This requirement of the network arises from the fact that the forward() method's in the nn.Conv2d convolutional layer\n",
    "classes expect their tenors to have 4 dimensions. This is pretty standard as most neural network implementations deal\n",
    "with batches of input samples rather than single samples."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(single_image.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To put our single sample image tensor into a batch with a size of 1, we just need to unsqueeze() the tensor to add an\n",
    "additional dimension."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(single_image.unsqueeze(dim=0).shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using this, we can now pass the unsqueezed image to our network and get the network's prediction."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "single_pred = network(\n",
    "    single_image.unsqueeze(dim=0)\n",
    ")\n",
    "print(single_pred.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The shape of the prediction tensor is 1 x 10. This tells us that the first axis has a length of one while the second\n",
    "axis has a length of ten. The interpretation of this is that we have one image in our batch and ten prediction classes.\n",
    "\n",
    "    · (batch size, number of prediction classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1331, grad_fn=<MaxBackward1>)\n",
      "Prediction: tensor([1])\n",
      "Actual label: 9\n"
     ]
    }
   ],
   "source": [
    "print(single_pred.max())\n",
    "print('Prediction:', single_pred.argmax(dim=1)) # dim=1 because we want to search horizontally (1, 10) by column!\n",
    "print('Actual label:', single_label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For each input in the batch, and for each prediction class, we have a prediction value. If we wanted these values to be\n",
    "probabilities, we could just the softmax() function from the nn.functional package."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Search for the predictions in axis=1 (horizontally), because the pred tensor has shape of (1, 10).\n",
    "Searching by axis=0 will result in 10 values, while we search for a single value."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1079, 0.1139, 0.1089, 0.0909, 0.1078, 0.0952, 0.0901, 0.0990, 0.1017,\n",
      "         0.0846]], grad_fn=<SoftmaxBackward>)\n",
      "tensor(1., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    F.softmax(single_pred, dim=1)\n",
    ")\n",
    "print(\n",
    "    F.softmax(single_pred, dim=1).sum()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The label for the first image in our training set is 9, and using the argmax() function we can see that the highest\n",
    "value in our prediction tensor occurred at the class represented by index 7.\n",
    "\n",
    "        · Prediction: Sneaker (7)\n",
    "        · Actual: Ankle boot (9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Network Weights Are Randomly Generated\n",
    "There are a couple of important things we need to point out about these results. Most of the probabilities came in close\n",
    "to 10%, and this makes sense because our network is guessing and we have ten prediction classes coming from a balanced\n",
    "dataset.\n",
    "\n",
    "Another implication of the randomly generated weights is that each time we create a new instance of our network, the\n",
    "weights within the network will be different. This means that the predictions we get will be different if we create\n",
    "different networks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Passing A Batch Of Images To The Network\n",
    "The data loader returns a batch of images that are packaged into a single tensor with a shape that reflects the\n",
    "following axes.\n",
    "\n",
    "    · (batch size, input channels, height, width)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alright. Let’s get a prediction by passing the images tensor to the network."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "preds = network(images)\n",
    "print(preds.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The prediction tensor has a shape of 10 by 10, which gives us two axes that each have a length of ten. This reflects the\n",
    "fact that we have ten images and for each of these ten images we have ten prediction classes.\n",
    "\n",
    "    · (batch size, number of prediction classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using Argmax: Prediction Vs Label\n",
    "To check the predictions against the labels, we use the argmax() function to figure out which index contains the highest\n",
    "prediction value. Once we know which index has the highest prediction value, we can compare the index with the label to\n",
    "see if there is a match.\n",
    "\n",
    "To do this, we call the argmax() function on the prediction tensor, and we specify second dimension.\n",
    "\n",
    "The second dimension is the last dimension of our prediction tensor. Remember from all of our work on tensors, the last\n",
    "dimension of a tensor always contains numbers while every other dimension contains other smaller tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1])\n",
      "tensor([2, 4, 3, 1, 3, 8, 9, 9, 8, 2])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(preds.argmax(dim=1))\n",
    "print(labels)\n",
    "\n",
    "print(\n",
    "    preds.argmax(dim=1).eq(labels).sum()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can wrap this last call into a function called get_num_correct() that accepts the predictions and the labels, and\n",
    "uses the item() method to return the Python number of correct predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def get_num_correct(y_pred, y_true):\n",
    "    return y_pred.argmax(dim=1).eq(y_true).sum().item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly predicted labels: 1 of 10\n"
     ]
    }
   ],
   "source": [
    "print('Number of correctly predicted labels:',\n",
    "    get_num_correct(preds, labels),\n",
    "      'of', len(labels)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The relu() Activation Function\n",
    "\n",
    "    · f(x) = 0 for x < 0 and x if x >= 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.18432478606700897\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    preds.min().item()\n",
    ")\n",
    "print(\n",
    "    F.relu(\n",
    "        preds.min()\n",
    "    ).item()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Max Pooling Operation\n",
    "The pooling operation reduces the shape of our tensor further by extracting the maximum value from each m x n location\n",
    "within our tensor.\n",
    "\n",
    "    t.shape\n",
    "        >> torch.Size([1, 6, 24, 24])\n",
    "\n",
    "    t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "    t.shape\n",
    "        >> torch.Size([1, 6, 12, 12])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convolution Layer Summary\n",
    "The shapes of the tensor input to and output from the convolutional layer is given by:\n",
    "\n",
    "    · Input shape: [1, 1, 28, 28]\n",
    "    · Output shape: [1, 6, 12, 12]\n",
    "\n",
    "Summary of each operation that occurs:\n",
    "\n",
    "    1.  The convolution layer convolves the input tensor using six randomly initialized 5x5 filters.\n",
    "    2.  This reduces the height and width dimensions by four.\n",
    "    3.  The relu activation function operation maps all negative values to 0.\n",
    "    4.  This means that all the values in the tensor are now positive.\n",
    "    5.  The max pooling operation extracts the max value from each 2x2 section of the six feature maps that were created by the convolutions.\n",
    "    6.  This reduced the height and width dimensions by twelve.\n",
    "\n",
    "CNN Output Size Formula (Square)\n",
    "\n",
    "output size = (n - f + 2p)/(s) + 1, where n is nxn input, f is fxf filter, p is padding and stride is s\n",
    "\n",
    "    self.conv2.weight.shape\n",
    "        >> torch.Size([12, 6, 5, 5])\n",
    "    t.shape\n",
    "        >> torch.Size([1, 6, 12, 12])\n",
    "\n",
    "    t = self.conv2(t)\n",
    "    t.shape\n",
    "        >> torch.Size([1, 12, 8, 8]) -> (12 - 5)/1 + 1 = 8, no zero padding using filer 5x5 and stride=1\n",
    "\n",
    "    > t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "    > t.shape\n",
    "    torch.Size([1, 12, 4, 4])\n",
    "\n",
    "The shape of the resulting output of self.conv2 allows us to see why we reshape the tensor using 12*4*4 before passing\n",
    "the tensor to the first linear layer, self.fc1.\n",
    "\n",
    "This table summarizes the shape changing operations and the resulting shape of each:\n",
    "\n",
    "    Operation\tOutput Shape\n",
    "    · Identity function\ttorch.Size([1, 1, 28, 28])\n",
    "    · Convolution (5 x 5)\ttorch.Size([1, 6, 24, 24]) # no zero padding, using filter 5x5 and stride=1\n",
    "    · Max pooling (2 x 2)\ttorch.Size([1, 6, 12, 12])\n",
    "    · Convolution (5 x 5)\ttorch.Size([1, 12, 8, 8])\n",
    "    · Max pooling (2 x 2)\ttorch.Size([1, 12, 4, 4])\n",
    "    · Flatten (reshape)\ttorch.Size([1, 192])\n",
    "    · Linear transformation\ttorch.Size([1, 120])\n",
    "    · Linear transformation\ttorch.Size([1, 60])\n",
    "    · Linear transformation\ttorch.Size([1, 10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training: What We Do After The Forward Pass\n",
    "\n",
    "    1.  Get batch from the training set.\n",
    "    2.  Pass batch to network.\n",
    "    3.  Calculate the loss (difference between the predicted values and the true values).\n",
    "    4.  Calculate the gradient of the loss function w.r.t the network's weights.\n",
    "    5.  Update the weights using the gradients to reduce the loss.\n",
    "    6.  Repeat steps 1-5 until one epoch is completed.\n",
    "    7.  Repeat steps 1-6 for as many epochs required to reach the minimum loss.\n",
    "We already know exactly how to do steps 1 and 2. If you've already covered the deep learning fundamentals series, then\n",
    "you know that we use a loss function to perform step 3, and you know that we use backpropagation and an optimization\n",
    "algorithm to perform step 4 and 5. Steps 6 and 7 are just standard Python loops (the training loop). Let's see how this\n",
    "is done in code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing For The Forward Pass\n",
    "We'll begin by:\n",
    "\n",
    "    1.  Creating an instance of our Network class.\n",
    "    2.  Creating a data loader that provides batches of size 100 from our training set.\n",
    "    3.  Unpacking the images and labels from one of these batches."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "network = Network()\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=100\n",
    ")\n",
    "batch = next(iter(train_loader)) # image set is divided into 60 000 / 100 batches\n",
    "images, labels = batch # batch of 100 images and 100 corresponding labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculating The Loss\n",
    "To do this we will use the cross_entropy() loss function that is available in PyTorch's nn.functional API. Once we have\n",
    "the loss, we can print it, and also check the number of correct predictions using the function we created a previous\n",
    "post."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "preds = network(images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.295222043991089\n"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(\n",
    "    input=preds,\n",
    "    target=labels\n",
    ")\n",
    "print(loss.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember that the values are located in the last dimension! In case of predictions (rank=2, dim=0, 1), the predicted\n",
    "labels are located on dim=1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    preds.argmax(dim=1).eq(labels).sum().item()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cross_entropy() function returned a scalar valued tenor, and so we used the item() method to print the loss as a\n",
    "Python number. We got 9 out of 100 correct, and since we have 10 prediction classes, this is what we'd expect by\n",
    "guessing at random."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculating The Gradients\n",
    "Before we calculate the gradients, let's verify that we currently have no gradients inside our conv1 layer. The\n",
    "gradients are tensors that are accessible in the grad (short for gradient) attribute of the weight tensor of each layer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    network.conv1.weight.grad is None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To calculate the gradients, we call the backward() method (Gradient is calculated during back propagation) method on\n",
    "the loss tensor, like so:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "loss.backward(retain_graph=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the gradients of the loss function have been stored inside weight tensors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    network.conv1.weight.grad.shape\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Updating The Weights\n",
    "To the Adam class constructor, we pass the network parameters (this is how the optimizer is able to access the\n",
    "gradients), and we pass the learning rate .\n",
    "\n",
    "Finally, all we have to do is update the weights is to tell the optimizer to use the gradients to step in the\n",
    "direction of the loss function's minimum."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(\n",
    "    network.parameters(),\n",
    "    lr=0.01\n",
    ")\n",
    "optimizer.step() # Updating weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When the step() function is called, the optimizer updates the weights using the gradients that are stored in the\n",
    "network's parameters. This means that we should expect our loss to be reduced if we pass the same batch through the\n",
    "network again. Checking this, we can see that this is indeed the case:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.266753673553467\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "preds = network(images)\n",
    "loss = F.cross_entropy(\n",
    "    input=preds,\n",
    "    target=labels\n",
    ")\n",
    "print(loss.item())\n",
    "print(\n",
    "    preds.argmax(dim=1).eq(labels).sum().item()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Using A Single Batch\n",
    "We can summarize the code for training with a single batch in the following way:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-optimized loss: tensor(2.2993, grad_fn=<NllLossBackward>)\n",
      "Optimized loss: tensor(2.2784, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    train=True,\n",
    "    root='./data',\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "batch = next(\n",
    "    iter(train_loader)\n",
    ")\n",
    "images, labels = batch\n",
    "\n",
    "predictions = network(images)\n",
    "\n",
    "loss_noptim = F.cross_entropy(\n",
    "    input=predictions,\n",
    "    target=labels\n",
    ")\n",
    "\n",
    "loss_noptim.backward() # Calculate gradients\n",
    "\n",
    "# Use to optimizer to update weights and predict one more time.\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=network.parameters(),\n",
    "    lr=0.01\n",
    ")\n",
    "optimizer.step() # Update weights (after calculating gradients by loss_noptim)\n",
    "\n",
    "predictions = network(images)\n",
    "\n",
    "loss = F.cross_entropy(\n",
    "    input=predictions,\n",
    "    target=labels\n",
    ")\n",
    "loss.backward() # Calculate gradients after optimization\n",
    "\n",
    "print('Non-optimized loss:', loss_noptim)\n",
    "print('Optimized loss:', loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss function is used to calculate the gradients.\n",
    "\n",
    "The gradients are then used by the optimizer to update the weights.\n",
    "\n",
    "After the weights are updated, the loss function will be reduced."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training With All Batches (Single Epoch)\n",
    "Now, to train with all of the batches available inside our data loader, we need to make a few changes and add one\n",
    "additional line of code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      "Total_correct: 46681 \n",
      "Total_loss: 347.30973121523857 \n",
      "Accuracy: 77.80166666666666 %\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")\n",
    "optimizer = optim.Adam(\n",
    "    params=network.parameters(),\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "total_loss = 0\n",
    "total_correct = 0\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, labels = batch\n",
    "\n",
    "    predictions = network(images)\n",
    "    loss = F.cross_entropy(\n",
    "        input=predictions,\n",
    "        target=labels\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "    total_correct += predictions.argmax(dim=1).eq(labels).sum().item()\n",
    "print(\n",
    "    'Epoch:', 0,\n",
    "    '\\nTotal_correct:', total_correct,\n",
    "    '\\nTotal_loss:', total_loss,\n",
    "    '\\nAccuracy:', total_correct/len(train_set) * 100, '%'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That's pretty good after only one epoch (a single full pass over the data). Even though we did one epoch, we still have\n",
    "to keep in mind that the weights were updated 600 times, and this fact depends on our batch size. If made our\n",
    "batch_batch size larger, say 10,000, the weights would only be updated 6 times, and the results wouldn't be quite as\n",
    "good.\n",
    "\n",
    "    Epoch - single iteration over whole dataset (if dataset=1000, batch_size=100, single epoch would constist of\n",
    "    10 iterations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In PyTorch, we need to set the gradients to zero before starting to do backpropragation, because PyTorch accumulates\n",
    "the gradients on subsequent backward passes. This is convenient while training RNNs. The default action is to\n",
    "accumulate (i.e. sum) the gradients on every loss.backward() call.\n",
    "\n",
    "Because of this, when you start your training loop, ideally you should zero out the gradients so that you do the\n",
    "parameter update correctly. Else the gradient would point in some other direction than the intended direction towards\n",
    "the minimum (or maximum, in case of maximization objectives).\n",
    "\n",
    "After we call the backward() method on our loss tensor, we know the gradients will be calculated and added to the grad\n",
    "attributes of our network's parameters. For this reason, we need to zero out these gradients.\n",
    "\n",
    "        1.  optimizer.zero_grad() - zero out the gradients\n",
    "        2.  loss.backward() - calculate the gradients using backpropagation\n",
    "        3.  optimizer.step() - update the weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training With Multiple Epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "network = Network()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=network.parameters(),\n",
    "    lr=0.01\n",
    ")\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=100,\n",
    "    shuffle=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just a quick review of what we do during training:\n",
    "\n",
    "    1.  Get batch from the training set.\n",
    "    2.  Pass batch to network.\n",
    "    3.  Calculate the loss (difference between the predicted values and the true values).\n",
    "    4.  Calculate the gradient of the loss function w.r.t the network's weights.\n",
    "    5.  Update the weights using the gradients to reduce the loss.\n",
    "    6.  Repeat steps 1-5 until one epoch is completed.\n",
    "    7.  Repeat steps 1-6 for as many epochs required to reach the minimum loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      "Total_correct: 47267 \n",
      "Total_loss: 338.7649166435003 \n",
      "Accuracy: 0.7877833333333333\n",
      "Epoch: 1 \n",
      "Total_correct: 51503 \n",
      "Total_loss: 229.65345931053162 \n",
      "Accuracy: 0.8583833333333334\n",
      "Epoch: 2 \n",
      "Total_correct: 52161 \n",
      "Total_loss: 209.96502673625946 \n",
      "Accuracy: 0.86935\n",
      "Epoch: 3 \n",
      "Total_correct: 52571 \n",
      "Total_loss: 200.80134943127632 \n",
      "Accuracy: 0.8761833333333333\n",
      "Epoch: 4 \n",
      "Total_correct: 52843 \n",
      "Total_loss: 195.2149475812912 \n",
      "Accuracy: 0.8807166666666667\n",
      "Epoch: 5 \n",
      "Total_correct: 53007 \n",
      "Total_loss: 188.78152999281883 \n",
      "Accuracy: 0.88345\n",
      "Epoch: 6 \n",
      "Total_correct: 53067 \n",
      "Total_loss: 187.06489123404026 \n",
      "Accuracy: 0.88445\n",
      "Epoch: 7 \n",
      "Total_correct: 53140 \n",
      "Total_loss: 185.62753687798977 \n",
      "Accuracy: 0.8856666666666667\n",
      "Epoch: 8 \n",
      "Total_correct: 53261 \n",
      "Total_loss: 181.0988917723298 \n",
      "Accuracy: 0.8876833333333334\n",
      "Epoch: 9 \n",
      "Total_correct: 53364 \n",
      "Total_loss: 177.99109853059053 \n",
      "Accuracy: 0.8894\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    total_loss_per_epoch = 0\n",
    "    total_correct_per_epoch = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "\n",
    "        images, labels = batch # 1. Get Batch\n",
    "\n",
    "        predictions = network(images) # 2. Pass Batch into Network\n",
    "\n",
    "        loss = nn.functional.cross_entropy(\n",
    "            input=predictions,\n",
    "            target=labels\n",
    "        ) # 3. Calculate Loss Function\n",
    "\n",
    "        optimizer.zero_grad() # 4A. Zero out the gradients\n",
    "        loss.backward() # 4B. Calculate the gradients\n",
    "        optimizer.step() # 5. Update the weights\n",
    "\n",
    "        total_loss_per_epoch += loss.item()\n",
    "        total_correct_per_epoch += predictions.argmax(dim=1).eq(labels).sum().item()\n",
    "        # 6. Iterate over all batches from the dataset\n",
    "\n",
    "    print(\n",
    "        'Epoch:', epoch,\n",
    "        '\\nTotal_correct:', total_correct_per_epoch,\n",
    "        '\\nTotal_loss:', total_loss_per_epoch,\n",
    "        '\\nAccuracy:', total_correct_per_epoch/len(train_set)\n",
    "    )\n",
    "    # 7. Iterate through next epochs until the loss function is minimized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get Predictions For The Entire Training Set\n",
    "depending on the computing resources and the size of the training set if we were training on a different data set, we\n",
    "need a way to prediction on smaller batches and collect the results. To collect the results, we'll use the torch.cat()\n",
    "function to concatenate the output tensors together to obtain our single prediction tensor. Let's build a function to\n",
    "do this."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building A Function To Get Predictions For ALL Samples\n",
    "All the function needs to do is iterate over the data loader passing the batches to the model and concatenating the\n",
    "results of each batch to a prediction tensor that will returned to the caller."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_predictions(network_model, data_loader):\n",
    "    all_predictions = torch.tensor([])\n",
    "    all_correct = 0\n",
    "    for batch in data_loader:\n",
    "        images, labels = batch\n",
    "        predictions = network_model(images)\n",
    "\n",
    "        all_predictions = torch.cat(\n",
    "            tensors=(all_predictions, predictions),\n",
    "            dim=0\n",
    "        )\n",
    "        all_correct += get_num_correct(predictions, labels)\n",
    "    return all_predictions, all_correct"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note at the top, we have annotated the function using the @torch.no_grad() PyTorch decoration. This is because we want\n",
    "this functions execution to omit gradient tracking.\n",
    "\n",
    "We specifically need the gradient calculation feature anytime we are going to calculate gradients using the backward()\n",
    "function. Otherwise, it is a good idea to turn it off because having it off will reduce memory consumption for\n",
    "computations, e.g. when we are using networks only for predicting (inference).\n",
    "\n",
    "We can disable gradient computations for specific or local spots in our code, e.g. like what we just saw with the\n",
    "annotated function ( function decorator @torch.no_grad() ).\n",
    "\n",
    "As another example, we can use Python's 'with' context manger keyword to specify that a specify block\n",
    "of code should exclude gradient computations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    big_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=10000,\n",
    "        shuffle=False\n",
    "    )\n",
    "    all_predictions, all_correct = get_all_predictions(network, big_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using The Predictions Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct: 53539\n",
      "Accuracy: 89.23166666666667%\n"
     ]
    }
   ],
   "source": [
    "print('Total Correct:', all_correct)\n",
    "print('Accuracy: {}%'.format((all_correct/len(train_set) * 100)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building The Confusion Matrix\n",
    "Our task in building the confusion matrix is to count the number of predicted values against the true values (targets).\n",
    "\n",
    "This will create a matrix that acts as a heat map telling us where the predicted values fall relative to the true\n",
    "values.\n",
    "\n",
    "To do this, we need to have the targets tensor and the predicted label from the train_preds tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "stacked_h = torch.stack(\n",
    "    tensors=(\n",
    "        train_set.targets, all_predictions.argmax(dim=1)\n",
    "    ),\n",
    "    dim=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9, 9],\n",
      "        [0, 0],\n",
      "        [0, 0],\n",
      "        ...,\n",
      "        [3, 3],\n",
      "        [0, 0],\n",
      "        [5, 5]])\n"
     ]
    }
   ],
   "source": [
    "print(stacked_h)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "cmt = torch.zeros(10, 10, dtype=torch.int64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(cmt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "for pred_true_pair in stacked_h:\n",
    "    pred, true = pred_true_pair.tolist()\n",
    "    cmt[pred, true] = cmt[pred, true] + 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5214,    5,  115,  165,   11,    1,  439,    0,   50,    0],\n",
      "        [  11, 5891,    1,   81,    6,    1,    2,    1,    6,    0],\n",
      "        [  56,    2, 5294,   86,  326,    1,  204,    0,   31,    0],\n",
      "        [ 116,   25,   14, 5570,  194,    1,   61,    0,   19,    0],\n",
      "        [   7,    9,  886,  180, 4757,    0,  139,    0,   22,    0],\n",
      "        [   0,    0,    6,    1,    0, 5888,    0,   79,    6,   20],\n",
      "        [ 887,    7,  691,  168,  679,    2, 3486,    0,   80,    0],\n",
      "        [   0,    0,    0,    0,    0,   47,    0, 5703,    2,  248],\n",
      "        [  10,    2,   23,   11,   12,    4,   24,    9, 5904,    1],\n",
      "        [   0,    0,    0,    1,    0,   42,    3,  117,    5, 5832]])\n"
     ]
    }
   ],
   "source": [
    "print(cmt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting Confusion Matrix\n",
    "To generate the actual confusion matrix as a numpy.ndarray, we use the confusion_matrix() function from the\n",
    "sklearn.metrics library. Let's get this imported along with our other needed imports."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "cm = confusion_matrix(\n",
    "    y_true=train_set.targets,\n",
    "    y_pred=all_predictions.argmax(dim=1),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5214    5  115  165   11    1  439    0   50    0]\n",
      " [  11 5891    1   81    6    1    2    1    6    0]\n",
      " [  56    2 5294   86  326    1  204    0   31    0]\n",
      " [ 116   25   14 5570  194    1   61    0   19    0]\n",
      " [   7    9  886  180 4757    0  139    0   22    0]\n",
      " [   0    0    6    1    0 5888    0   79    6   20]\n",
      " [ 887    7  691  168  679    2 3486    0   80    0]\n",
      " [   0    0    0    0    0   47    0 5703    2  248]\n",
      " [  10    2   23   11   12    4   24    9 5904    1]\n",
      " [   0    0    0    1    0   42    3  117    5 5832]]\n"
     ]
    }
   ],
   "source": [
    "print(cm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "from plot_confusion_matrix import plot_confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[5214    5  115  165   11    1  439    0   50    0]\n",
      " [  11 5891    1   81    6    1    2    1    6    0]\n",
      " [  56    2 5294   86  326    1  204    0   31    0]\n",
      " [ 116   25   14 5570  194    1   61    0   19    0]\n",
      " [   7    9  886  180 4757    0  139    0   22    0]\n",
      " [   0    0    6    1    0 5888    0   79    6   20]\n",
      " [ 887    7  691  168  679    2 3486    0   80    0]\n",
      " [   0    0    0    0    0   47    0 5703    2  248]\n",
      " [  10    2   23   11   12    4   24    9 5904    1]\n",
      " [   0    0    0    1    0   42    3  117    5 5832]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1080x1080 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(\n",
    "    cm=cm,\n",
    "    classes=train_set.classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interpreting The Confusion Matrix\n",
    "The confusion matrix has three axes:\n",
    "\n",
    "    1.  Prediction label (class)\n",
    "    2.  True label\n",
    "    3.  Heat map value (color)\n",
    "\n",
    "The prediction label and true labels show us which prediction class we are dealing with. The matrix diagonal represents\n",
    "locations in the matrix where the prediction and the truth are the same, so this is where we want the heat map to be\n",
    "darker.\n",
    "\n",
    "Any values that are not on the diagonal are incorrect predictions because the prediction and the true label don't match.\n",
    "To read the plot, we can use these steps:\n",
    "\n",
    "    1.  Choose a prediction label on the horizontal axis.\n",
    "    2.  Check the diagonal location for this label to see the total number correct.\n",
    "    3.  Check the other non-diagonal locations to see where the network is confused."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}