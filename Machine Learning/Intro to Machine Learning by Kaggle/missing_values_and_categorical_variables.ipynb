{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Numerical Variables\n",
    "# Dealing with missing values\n",
    "There are many ways data can end up with missing values.\n",
    "Most machine learning libraries (including scikit-learn) give an error if you try to build a model using data with\n",
    "missing values.\n",
    "\n",
    "1)  A simple option: Drop columns or rows with missing values\n",
    "\n",
    "    Unless most values in the dropped columns are missing, the model loses access to a lot of (potentially useful!)\n",
    "    information with this approach. As an extreme example, consider a dataset with 10,000 rows, where one important\n",
    "    column is missing a single entry. This approach would drop the column entirely!\n",
    "\n",
    "\n",
    "2) A better option: Imputation\n",
    "\n",
    "    Imputation fills in the missing values with some number. For instance, we can fill in the mean value along each\n",
    "    column.\n",
    "\n",
    "    The imputed value won't be exactly right in most cases, but it usually leads to more accurate models than you would\n",
    "    get from dropping the column entirely.\n",
    "\n",
    "3) An Extension to Imputation\n",
    "\n",
    "    Imputation is the standard approach, and it usually works well. However, imputed values may be systematically\n",
    "    above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be\n",
    "    unique in some other way. In that case, your model would make better predictions by considering which values\n",
    "    were originally missing.\n",
    "\n",
    "    In this approach, we impute the missing values, as before. And, additionally, for each column with missing\n",
    "    entries in the original dataset, we add a new column that shows the location of the imputed entries.\n",
    "\n",
    "    In some cases, this will meaningfully improve results. In other cases, it doesn't help at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [],
   "source": [
    "iowa_data = pd.read_csv('data/Melbourne_housing_FULL.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG',\n",
      "       'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car',\n",
      "       'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude',\n",
      "       'Longtitude', 'Regionname', 'Propertycount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(iowa_data.columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "outputs": [],
   "source": [
    "features = ['Rooms', 'Bathroom', 'Car', 'Longtitude', 'Lattitude', 'Landsize']\n",
    "X = iowa_data[features]\n",
    "y = iowa_data['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=37)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Function to Measure Quality of Each Approach\n",
    "We define a function score_dataset() to compare different approaches to dealing with missing values. This takes in the\n",
    "reduced feature matrices and returns the mean absolute error (MAE) from a random forest model.\n",
    "\n",
    "The purpose is to see, which features have high impact on the accurate target predictions.\n",
    "\n",
    "To achieve different results, we train our model on different feature matrices, while maintaining the target matrix's\n",
    "size unchanged."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [],
   "source": [
    "def mae(X_train, X_test, y_train, y_test):\n",
    "    model = RandomForestRegressor(random_state=17)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return mean_absolute_error(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 1: Drop columns with missing values\n",
    "This approach will not work for me, because I have NaN's in the Price columns. I will drop rows where the Nan's are\n",
    "present."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rooms', 'Price', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'Lattitude', 'Longtitude', 'Propertycount']\n",
      "['Rooms', 'Bathroom', 'Car', 'Longtitude', 'Lattitude', 'Landsize']\n"
     ]
    }
   ],
   "source": [
    "all_numeric = [column_name for column_name in iowa_data if is_numeric_dtype(iowa_data[column_name])]\n",
    "print(all_numeric)\n",
    "print(features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 1: Drop columns or rows\n",
    "I will drop rows with missing values\n",
    "Remember that the reduced dataframe must contain the same rows in features and 'Price' columns!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "All features are feature and target columns together."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [],
   "source": [
    "all_features = features + ['Price']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [],
   "source": [
    "reduced_iowa_data = iowa_data[all_features].dropna(axis=0, how='any')\n",
    "reduced_X = reduced_iowa_data[features]\n",
    "reduced_y = reduced_iowa_data['Price']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The idea behind the underlaying operations is to have the same training and validation sets as the default X and y,\n",
    "except rows in feature and target columns, which contain Nans at different indexes. That's why we need to delete the\n",
    "entries with the same index (row) in both train/test X and y in order to maintain the proper shape of matrices."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [],
   "source": [
    "reduced_train_indices = [index for index, row in X_train.iterrows() if index not in reduced_X.index]\n",
    "reduced_X_train = X_train.copy().drop(reduced_train_indices, axis=0)\n",
    "reduced_y_train = y_train.copy().drop(reduced_train_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "outputs": [],
   "source": [
    "reduced_test_indices = [index for index, row in X_test.iterrows() if index not in reduced_X.index]\n",
    "reduced_X_test = X_test.copy().drop(reduced_test_indices, axis=0)\n",
    "reduced_y_test = y_test.copy().drop(reduced_test_indices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of reduced train sets are the same - necessary condition fulfilled.\n",
      "Indexes of reduced test sets are the same - necessary condition fulfilled.\n"
     ]
    }
   ],
   "source": [
    "if all(reduced_X_train.index == reduced_y_train.index):\n",
    "    print('Indexes of reduced train sets are the same - necessary condition fulfilled.')\n",
    "if all(reduced_X_test.index == reduced_y_test.index):\n",
    "    print('Indexes of reduced test sets are the same - necessary condition fulfilled.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for Approach 1:\n",
      " 172810.41165174177\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error for Approach 1:\\n',\n",
    "      mae(reduced_X_train, reduced_X_test, reduced_y_train, reduced_y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 2: Imputation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_iowa_data = pd.DataFrame(data=imputer.fit_transform(iowa_data[all_features]), columns=all_features)\n",
    "imputed_X = imputed_iowa_data[features]\n",
    "imputed_y = imputed_iowa_data['Price']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember to use fit_transform with the train sets, and only transform with test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [],
   "source": [
    "imputed_X_train = pd.DataFrame(data=imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "imputed_X_test = pd.DataFrame(data=imputer.transform(X_test), columns=X_test.columns)\n",
    "imputed_y_train = pd.DataFrame(data=imputer.fit_transform(y_train.values.reshape(-1, 1)), columns=['Price'])\n",
    "imputed_y_test = pd.DataFrame(data=imputer.transform(y_test.values.reshape(-1, 1)), columns=['Price'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for Approach 2:\n",
      " 261683.4368014598\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error for Approach 2:\\n',\n",
    "      mae(imputed_X_train, imputed_X_test, imputed_y_train, imputed_y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although it's simple, filling in the mean value generally performs quite well (but this varies by dataset). While\n",
    "statisticians have experimented with more complex ways to determine imputed values (such as regression imputation,\n",
    "for instance), the complex strategies typically give no additional benefit once you plug the results into sophisticated\n",
    "machine learning models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 3: An Extension to Imputation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Firstly check for column names (all_features=feature columns + target column), which contain Nans."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bathroom', 'Car', 'Longtitude', 'Lattitude', 'Landsize']\n"
     ]
    }
   ],
   "source": [
    "columns_with_nans = [column_name for column_name in iowa_data[features] if any(iowa_data[column_name].isnull())]\n",
    "print(columns_with_nans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a copy of DataFrame, so you do not overwrite original DataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [],
   "source": [
    "imputed_X_train_extended = X_train.copy()\n",
    "imputed_X_test_extended = X_test.copy()\n",
    "imputed_y_train_extended = y_train.copy()\n",
    "imputed_y_test_extended = y_test.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create columns with names COLUMN_WAS_NAN, meaning that a column previously contained at least one NaN, and values in\n",
    "corresponding columns will be imputed/deleted."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [],
   "source": [
    "for column_name in columns_with_nans:\n",
    "    imputed_X_train_extended[column_name + '_was_NaN'] = imputed_X_train_extended[column_name].isnull()\n",
    "    imputed_X_test_extended[column_name + '_was_NaN'] = imputed_X_test_extended[column_name].isnull()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [],
   "source": [
    "imputed_y_train_extended = pd.DataFrame(data=y_train, columns=['Price'])\n",
    "imputed_y_train_extended['Price_was_NaN'] = imputed_y_train_extended['Price'].isnull()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [],
   "source": [
    "imputed_y_test_extended = pd.DataFrame(data=y_test, columns=['Price'])\n",
    "imputed_y_test_extended['Price_was_NaN'] = imputed_y_test_extended['Price'].isnull()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [],
   "source": [
    "imputed_X_train_extended = pd.DataFrame(imputer.fit_transform(X_train))\n",
    "imputed_X_test_extended = pd.DataFrame(imputer.transform(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [],
   "source": [
    "imputed_y_train_extended = pd.DataFrame(imputer.fit_transform(y_train.values.reshape(-1, 1)))\n",
    "imputed_y_test_extended = pd.DataFrame(imputer.fit_transform(y_test.values.reshape(-1, 1)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\PythonGPU\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for Approach 3:\n",
      " 261622.55780141085\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error for Approach 3:\\n',\n",
    "      mae(imputed_X_train_extended, imputed_X_test_extended, imputed_y_train_extended, imputed_y_test_extended))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Categorical Variables\n",
    "A categorical variable takes only a limited number of values."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dealing with categorical variables"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approaches:\n",
    "    1) Drop Categorical variables\n",
    "\n",
    "        The easiest approach to dealing with categorical variables is to simply remove them from the dataset. This\n",
    "        approach will only work well if the columns did not contain useful information.\n",
    "\n",
    "    2) Label encoding\n",
    "\n",
    "        Label encoding assigns each unique value to a different integer.\n",
    "        This approach assumes an ordering of the categories:\n",
    "\n",
    "        \"Never\" (0) < \"Rarely\" (1) < \"Most days\" (2) < \"Every day\" (3).\n",
    "\n",
    "        This assumption makes sense in this example, because there is an indisputable ranking to the categories. Not all\n",
    "        categorical variables have a clear ordering in the values, but we refer to those that do as ordinal variables.\n",
    "        For tree-based models (like decision trees and random forests), you can expect label encoding to work well with\n",
    "        ordinal variables.\n",
    "\n",
    "     3) One-Hot Encoding\n",
    "\n",
    "        One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the\n",
    "        original data. To understand this, we'll work through an example.\n",
    "\n",
    "        Color: red, red, yellow, green, yellow -> red: 1 1 0 0 0, yellow: 0 0 1 0 1, green: 0 0 0 1 0\n",
    "\n",
    "        In contrast to label encoding, one-hot encoding does not assume an ordering of the categories. Thus, you can\n",
    "        expect this approach to work particularly well if there is no clear ordering in the categorical data (e.g.,\n",
    "        \"Red\" is neither more nor less than \"Yellow\"). We refer to categorical variables without an intrinsic ranking\n",
    "        as nominal variables.\n",
    "\n",
    "        One-hot encoding generally does not perform well if the categorical variable takes on a large number of values\n",
    "        (i.e., you generally won't use it for variables taking more than 15 different values)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "outputs": [
    {
     "data": {
      "text/plain": "       Suburb             Address  Rooms Type      Price Method SellerG  \\\n0  Abbotsford       68 Studley St      2    h        NaN     SS  Jellis   \n1  Abbotsford        85 Turner St      2    h  1480000.0      S  Biggin   \n2  Abbotsford     25 Bloomburg St      2    h  1035000.0      S  Biggin   \n3  Abbotsford  18/659 Victoria St      3    u        NaN     VB  Rounds   \n4  Abbotsford        5 Charles St      3    h  1465000.0     SP  Biggin   \n\n        Date  Distance  Postcode  ...  Bathroom  Car  Landsize  BuildingArea  \\\n0  3/09/2016       2.5    3067.0  ...       1.0  1.0     126.0           NaN   \n1  3/12/2016       2.5    3067.0  ...       1.0  1.0     202.0           NaN   \n2  4/02/2016       2.5    3067.0  ...       1.0  0.0     156.0          79.0   \n3  4/02/2016       2.5    3067.0  ...       2.0  1.0       0.0           NaN   \n4  4/03/2017       2.5    3067.0  ...       2.0  0.0     134.0         150.0   \n\n   YearBuilt         CouncilArea Lattitude  Longtitude             Regionname  \\\n0        NaN  Yarra City Council  -37.8014    144.9958  Northern Metropolitan   \n1        NaN  Yarra City Council  -37.7996    144.9984  Northern Metropolitan   \n2     1900.0  Yarra City Council  -37.8079    144.9934  Northern Metropolitan   \n3        NaN  Yarra City Council  -37.8114    145.0116  Northern Metropolitan   \n4     1900.0  Yarra City Council  -37.8093    144.9944  Northern Metropolitan   \n\n  Propertycount  \n0        4019.0  \n1        4019.0  \n2        4019.0  \n3        4019.0  \n4        4019.0  \n\n[5 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Suburb</th>\n      <th>Address</th>\n      <th>Rooms</th>\n      <th>Type</th>\n      <th>Price</th>\n      <th>Method</th>\n      <th>SellerG</th>\n      <th>Date</th>\n      <th>Distance</th>\n      <th>Postcode</th>\n      <th>...</th>\n      <th>Bathroom</th>\n      <th>Car</th>\n      <th>Landsize</th>\n      <th>BuildingArea</th>\n      <th>YearBuilt</th>\n      <th>CouncilArea</th>\n      <th>Lattitude</th>\n      <th>Longtitude</th>\n      <th>Regionname</th>\n      <th>Propertycount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Abbotsford</td>\n      <td>68 Studley St</td>\n      <td>2</td>\n      <td>h</td>\n      <td>NaN</td>\n      <td>SS</td>\n      <td>Jellis</td>\n      <td>3/09/2016</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>126.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Yarra City Council</td>\n      <td>-37.8014</td>\n      <td>144.9958</td>\n      <td>Northern Metropolitan</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Abbotsford</td>\n      <td>85 Turner St</td>\n      <td>2</td>\n      <td>h</td>\n      <td>1480000.0</td>\n      <td>S</td>\n      <td>Biggin</td>\n      <td>3/12/2016</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>202.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Yarra City Council</td>\n      <td>-37.7996</td>\n      <td>144.9984</td>\n      <td>Northern Metropolitan</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Abbotsford</td>\n      <td>25 Bloomburg St</td>\n      <td>2</td>\n      <td>h</td>\n      <td>1035000.0</td>\n      <td>S</td>\n      <td>Biggin</td>\n      <td>4/02/2016</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>156.0</td>\n      <td>79.0</td>\n      <td>1900.0</td>\n      <td>Yarra City Council</td>\n      <td>-37.8079</td>\n      <td>144.9934</td>\n      <td>Northern Metropolitan</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Abbotsford</td>\n      <td>18/659 Victoria St</td>\n      <td>3</td>\n      <td>u</td>\n      <td>NaN</td>\n      <td>VB</td>\n      <td>Rounds</td>\n      <td>4/02/2016</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Yarra City Council</td>\n      <td>-37.8114</td>\n      <td>145.0116</td>\n      <td>Northern Metropolitan</td>\n      <td>4019.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abbotsford</td>\n      <td>5 Charles St</td>\n      <td>3</td>\n      <td>h</td>\n      <td>1465000.0</td>\n      <td>SP</td>\n      <td>Biggin</td>\n      <td>4/03/2017</td>\n      <td>2.5</td>\n      <td>3067.0</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>134.0</td>\n      <td>150.0</td>\n      <td>1900.0</td>\n      <td>Yarra City Council</td>\n      <td>-37.8093</td>\n      <td>144.9944</td>\n      <td>Northern Metropolitan</td>\n      <td>4019.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 21 columns</p>\n</div>"
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iowa_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 1: Drop categorical columns - exclude data of type 'object'.\n",
    "Non-numeric data types are classified as 'object' dtypes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "outputs": [],
   "source": [
    "all_iowa_reduced = iowa_data.dropna(axis=0, how='any')\n",
    "all_X_reduced = all_iowa_reduced.loc[:, iowa_data.columns != 'Price']\n",
    "all_y_reduced = all_iowa_reduced['Price']\n",
    "all_X_train_reduced, all_X_test_reduced, all_y_train_reduced, all_y_test_reduced = train_test_split(\n",
    "    all_X_reduced,\n",
    "    all_y_reduced,\n",
    "    random_state=33)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "outputs": [],
   "source": [
    "num_X_train = all_X_train_reduced.select_dtypes(exclude=['object'])\n",
    "num_X_test = all_X_test_reduced.select_dtypes(exclude=['object'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for Approach 1:\n",
      " 172663.12860636064\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error for Approach 1:\\n',\n",
    "      mae(num_X_train, num_X_test, all_y_train_reduced, all_y_test_reduced))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 2: Use label encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_X_train = all_X_train_reduced.copy()\n",
    "label_X_test = all_X_test_reduced.copy()\n",
    "label_y_train = all_y_train_reduced.copy()\n",
    "label_y_test = all_y_test_reduced.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find columns that contain non-numeric data types."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [],
   "source": [
    "categorical_columns = [col for col in all_X_reduced if not is_numeric_dtype(all_X_reduced[col])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In X_train and X_test there may be different labels. We fit the training data, but not the test data. The labels, that\n",
    "are in test data, must also be included in train data!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "outputs": [],
   "source": [
    "def clean_train_test_labels(label_X_train, label_X_test, label_y_test, categorical_columns):\n",
    "    for col in label_X_train[categorical_columns]:\n",
    "        unique_labels_in_column = label_X_train[col].unique()\n",
    "        for index, val in label_X_test[col].items():\n",
    "            # Check if label from X_test is in X_train\n",
    "            if val not in unique_labels_in_column:\n",
    "                # If label is not present in training data, delete that row from both feature and target test sets.\n",
    "                label_X_test = label_X_test.drop(index, axis=0)\n",
    "                label_y_test = label_y_test.drop(index)\n",
    "    return label_X_test, label_y_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [],
   "source": [
    "label_X_test, label_y_test = clean_train_test_labels(\n",
    "    label_X_train,\n",
    "    label_X_test,\n",
    "    label_y_test,\n",
    "    categorical_columns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If label encoding will work, it means that all labels from X_test are in X_train.\n",
    "Otherwise, clean up the X_test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [],
   "source": [
    "for col in [col for col in all_X_reduced if not is_numeric_dtype(all_X_reduced[col])]:\n",
    "    label_X_train[col] = label_encoder.fit_transform(label_X_train[col])\n",
    "    label_X_test[col] = label_encoder.transform(label_X_test[col])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for Approach 2:\n",
      " 200464.45642857143\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error for Approach 2:\\n',\n",
    "      mae(label_X_train, label_X_test, label_y_train, label_y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approach 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[315, 8764, 3, 5, 250, 77, 33, 8]\n"
     ]
    }
   ],
   "source": [
    "object_nunique = list(map(lambda col: all_X_reduced[col].nunique(), categorical_columns))\n",
    "print(object_nunique)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "OneHotEncoder removes index, so remember to put it back!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "oh_X_train_cols_1 = pd.DataFrame(\n",
    "    data=one_hot_encoder.fit_transform(all_X_train_reduced[categorical_columns]),\n",
    "    index=all_X_train_reduced.index\n",
    ")\n",
    "oh_X_test_cols_1 = pd.DataFrame(\n",
    "    data=one_hot_encoder.transform(all_X_test_reduced[categorical_columns]),\n",
    "    index=all_X_test_reduced.index,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove categorical columns (will replace with one-hot encoding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "outputs": [],
   "source": [
    "oh_num_X_train_cols_1 = all_X_train_reduced.drop(categorical_columns, axis=1)\n",
    "oh_num_X_test_cols_1 = all_X_test_reduced.drop(categorical_columns, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add one-hot encoded columns to numerical train, test sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "outputs": [],
   "source": [
    "oh_X_train_1 = pd.concat([oh_num_X_train_cols_1, oh_X_train_cols_1], axis=1)\n",
    "oh_X_test_1 = pd.concat([oh_num_X_test_cols_1, oh_X_test_cols_1], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for Approach 3:\n",
      " 168834.8643249325\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error for Approach 3:\\n',\n",
    "      mae(oh_X_train_1, oh_X_test_1, all_y_train_reduced, all_y_test_reduced))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is better not to use hot encoding if number of cardinals (unique entries) in the column exceeds 15."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create dictionary with creadentials and corresponding columns.\n",
    "Firstly map the numbers of unique entries in all categorical columns.\n",
    "Then zip them only if number of entries is less than 15."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "outputs": [],
   "source": [
    "unique_categorical = list(map(lambda col: all_X_reduced[col].nunique(), categorical_columns))\n",
    "credentials = dict((key, val) for key, val in zip(\n",
    "    all_X_reduced[categorical_columns], unique_categorical) if val <= 15)\n",
    "columns_to_hot_encode = credentials.keys()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "outputs": [],
   "source": [
    "oh_X_train_cols_2 = pd.DataFrame(\n",
    "    data=one_hot_encoder.fit_transform(all_X_train_reduced[columns_to_hot_encode]),\n",
    "    index=all_X_train_reduced.index\n",
    ")\n",
    "oh_X_test_cols_2 = pd.DataFrame(\n",
    "    data=one_hot_encoder.fit_transform(all_X_test_reduced[columns_to_hot_encode]),\n",
    "    index=all_X_test_reduced.index\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "outputs": [],
   "source": [
    "oh_num_X_train_cols_2 = all_X_train_reduced.drop(categorical_columns, axis=1)\n",
    "oh_num_X_test_cols_2 = all_X_test_reduced.drop(categorical_columns, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "outputs": [],
   "source": [
    "oh_X_train_2 = pd.concat([oh_num_X_train_cols_2, oh_X_train_cols_2], axis=1)\n",
    "oh_X_test_2 = pd.concat([oh_num_X_test_cols_2, oh_X_test_cols_2], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error for Approach 3.1:\n",
      " 169864.38546268913\n"
     ]
    }
   ],
   "source": [
    "print('Mean Absolute Error for Approach 3.1:\\n',\n",
    "      mae(oh_X_train_2, oh_X_test_2, all_y_train_reduced, all_y_test_reduced))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}